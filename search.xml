<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[InnoDB存储引擎（一）- InnoDB体系结构]]></title>
    <url>%2F2019%2F08%2F02%2FInnoDB%2F</url>
    <content type="text"><![CDATA[InnoDB体系结构 后台线程内存InnoDB关键特性InnoDB存储引擎关键特性： 插入缓冲 两次写 自适应哈希索引 异步IO 刷新邻接页 插入缓冲Insert Buffer两次写 doublewrite由两部分组成，一部分是内存中的doublewrite buffer，大小为2MB，另一部分是物理磁盘上共享表空间中连续128个页，大小同样为2MB。 Mysql技术内幕（InnoDB存储引擎）]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InnoDB存储引擎（二）--InnoDB存储引擎中的锁]]></title>
    <url>%2F2019%2F08%2F02%2Fmysql%E9%94%81%2F</url>
    <content type="text"><![CDATA[锁的类型InnoDB存储引擎实现了如下两种标准的行级锁： 共享锁(SLock),允许事务读一行数据。 排他锁(XLock),允许事务删除或更新一行数据。 X S X 不兼容 不兼容 S 不兼容 兼容 此外，InnoDB存储引擎支持多粒度(granular)锁定，这种锁定允许事务在行级上的锁和表级上的锁同时存在。为了支持在不同粒度上进行加锁操作，InnoDB存储引擎支持一种额外的锁方式，称之为意向锁(Intention Lock)。意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更细粒度(fine granularity)上进行加锁。 一致性非锁定读致性的非锁定读(consistent nonlocking read)是指InnoDB存储引繁通过行多版本控制(multi versioning)的方式来读取当前执行时间数据库中行的数据。如果读取的行正在执行DELETE或UPDATE操作，这时读取操作不会因此去等待行上锁的释放。相反地，InnoDB存储引擎会去读取行的一个快照数据。 快照数据是指该行的之前版本的数据，该实现是通过undo段来完成。而undo用来在事务中回滚数据，因此快照数据本身是没有额外的开销。此外，读取快照数据是不要上锁的，因为没有事务需要对历史的数据进行修改操作。可以看到，非锁定读机制极大地提髙了数据库的并发性。在InnoDB存储引擎的默认设置下，这是默认的读取方式，即读取不会占用和等待表上的锁。 但是在不同事务隔离级别下，读取的方式不同，并不是在每个事务隔离级别下都是采用非锁定的一致性读。在事务隔离级别READ COMMITTED和REPEATABLE READ (InnoDB存储引擎的默认事务隔离级別)下，InnoDB存储引擎使用非锁定的一致性读。然而，对于快照数据的定义却不相同。在READ COMMITTED事务隔离级别下，对于快照数据，非一致性读总是读取被锁定行的最新一份快照数据。而在REPEATABLE READ事务隔离级别下，对于快照数据，非一致性读总是读取事务开始时的行数据版本。 MVCC的实现过程版本号 系统版本号：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。 事务版本号：事务开始时的系统版本号。 隐藏的列 MVCC 在每行记录后面都保存着两个隐藏的列，用来存储两个版本号： 创建版本号：指示创建一个数据行的快照时的系统版本号； 删除版本号：如果该快照的删除版本号大于当前事务版本号表示该快照有效，否则表示该快照已经被删除了。 以下实现过程针对可重复读隔离级别。 当开始一个事务时，该事务的版本号肯定大于当前所有数据行快照的创建版本号，理解这一点很关键。数据行快照的创建版本号是创建数据行快照时的系统版本号，系统版本号随着创建事务而递增，因此新创建一个事务时，这个事务的系统版本号比之前的系统版本号都大，也就是比所有数据行快照的创建版本号都大。 SELECT 多个事务必须读取到同一个数据行的快照，并且这个快照是距离现在最近的一个有效快照。但是也有例外，如果有一个事务正在修改该数据行，那么它可以读取事务本身所做的修改，而不用和其它事务的读取结果一致。 把没有对一个数据行做修改的事务称为 T，T 所要读取的数据行快照的创建版本号必须小于等于 T 的版本号，因为如果大于 T 的版本号，那么表示该数据行快照是其它事务的最新修改，因此不能去读取它。除此之外，T 所要读取的数据行快照的删除版本号必须是未定义或者大于 T 的版本号，因为如果小于等于 T 的版本号，那么表示该数据行快照是已经被删除的，不应该去读取它。 INSERT将当前系统版本号作为数据行快照的创建版本号。 DELETE将当前系统版本号作为数据行快照的删除版本号。 UPDATE将当前系统版本号作为更新前的数据行快照的删除版本号，并将当前系统版本号作为更新后的数据行快照的创建版本号。可以理解为先执行 DELETE 后执行 INSERT。 一致性锁定读InnoDB存储引擎对于select支持两种一致性的锁定读(locking read)操作： SELECT — FOR UPDATE SELECT — LOCK IN SHARE MODE SELECT-FOR UPDATE对读取的行记录加一个X锁，其他事务不能对已锁定的行加上任何锁。SELECT-LOCK IN SHARE MODE对读取的行记录加一个S锁，其他事务可以向被锁定的行加S锁，但是如果加X锁，则会被阻塞。 锁的算法行锁的三种算法InnoDB存储引擎有3种行锁的算法，其分别是: Record Lock：单个行记录上的锁 Gap Lock：间隙锁，锁定一个范围，但不包含记录本身 Next-Key Lock : Gap Lock+Record Lock,锁定一个范围，并且锁定记录本身 Record Lock总是会去锁住索引记录，如果InnoDB存储引擎表在建立的时候没有设置任何一个索引，那么这时InnoDB存储引擎会使用隐式的主键来进行锁定。Next-Key Lock 是结合了 Gap Lock 和 Record Lock 的一种锁定算法，在 Next-Key Lock算法下，InnoDB对于行的査询都是采用这种锁定算法。 例如一个索引有10, 11，13和20这四个值，那么该索引可能被Next-Key Locking的区间为： (-00 ,10] (10,11] (11， 13] (13, 20] (20,+ ~) 当査询的索引含有唯一属性时，InnoDB存储引擎会对Next-Key Lock进行优化，将其降级为Record Lock,即仅锁住索引本身，而不是范围。（仅适用于查询的列是唯一索引的情况） 若唯一索引由多个列组成，而査询仅是査找多个唯一索引列中的其中一个，那么査询其实是range类型查询，而不是point类型查询，故InnoDB存储引擎依然使用Next-Key Lock进行锁定。 幻读问题Phantom Problem是指在同一事务下，连续执行两次同样的SQL语句可能导致不同的结果，第二次的SQL语句可能会返回之前不存在的行。 InnoDB存储引擎默认的事务隔离级别是REPEATABLE READ,在该隔离级别下,其采用Next-Key Locking的方式来加锁。而在事务隔离级别READ COMMITTED下,其仅采用Record Lock。 事务并发一致性问题丢失修改T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。 读脏数据T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 不可重复读T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 幻影读T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 产生并发不一致性问题主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。 事务隔离级别 未提交读（READ UNCOMMITTED） —事务中的修改，即使没有提交，对其它事务也是可见的。 提交读（READ COMMITTED） — 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。 可重复读（REPEATABLE READ）—保证在同一个事务中多次读取同样数据的结果是一样的。 可串行化（SERIALIZABLE）–强制事务串行执行。 隔离级别 脏读 不可重复读 幻影读 未提交读 √ √ √ 提交读 × √ √ 可重复读 × × √ 可串行化 × × × 阻塞因为不同锁之间的兼容性关系，在有些时刻一个事务中的锁需要等待另一个事务中的锁释放它所占用的资源，这就是阻塞。阻塞并不是一件坏事，其是为了确保事务可以并发且正常地运行。 在InnoDB存储引擎中，参数innodb_lock_wait_timeout用来控制等待的时间(默认是50秒)，innodb_rollback_on_timeout用来设定是否在等待超时时对进行中的事务进行回滚操作(默认是OFF,代表不回滚)。 在默认情况下InnoDB存储引擎不会回滚超时引发的错误异常。其实InnoDB存储引擎在大部分情况下都不会对异常进行回滚。 锁升级锁升级(Lock Escalation)是指将当前锁的粒度降低。举例来说，数据库可以把一个表的1000个行锁升级为一个页锁，或者将页锁升级为表锁。如果在数据库的设计中认为锁是一种稀有资源，而且想避免锁的开销，那数据库中会频繁出现锁升级现象。 InnoDB存储引擎不存在锁升级的问题。因为其不是根据每个记录来产生行锁的，相反，其根据每个事务访问的每个页对锁进行管理的，采用的是位图的方式。因此不管一个事务锁住页中一个记录还是多个记录，其开销通常都是一致的。 MySQL技术内幕（InnoDB存储引擎） CS-Note]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式一致性协议]]></title>
    <url>%2F2019%2F07%2F19%2FPaxos%2F</url>
    <content type="text"><![CDATA[Paxos算法Paxos算法核心是一个一致性算法。在该一致性算法中，有三种参与角色，Proposer，Acceptor和Learner。 选定提案算法流程： 阶段一 Proposer选择一个提案编号K，然后想Acceptor的某个超过半数的子集成员发送编号为K的Prepare请求。 如果一个Acceptor收到一个编号为K的Prepare请求，如果K大于该Acceptor已经响应的所有Prepare请求的编号，那么它就会将已经批准过的最大编号提案作为响应反馈给Proposer；如果该Acceptor之前未批准过提案，那么直接返回空响应。该Acceptor承诺不会再批准小于K的提案，设置接收的提案值为K。 阶段二： 如果Proposer收到半数以上的Acceptor对于其发出的编号为K的Prepare请求响应，那么它就会发送一个针对[K,V]提案的Accept请求给Acceptor。(V为返回的所有响应中编号最大提案的值) 如果Acceptor收到这个[K,V]提案的Accept请求，只要该Acceptor尚未对编号大于K的Prepare请求作出响应，它就可以通过提案。 提案的获取方案一 一旦Acceptor批准了一个提案，就将该提案发送给所有Learner。需要让每个Acceptor与所有Learner逐个进行通信，通信次数至少为二者乘积。 方案二 所有的Acceptor将提案批准情况统一发送给一个特定的Learner，它来负责通知其他的Learner。问题：主Learner随时可能出现故障 方案三 Acceptor将批准的天发送给特定的Learner集合 实例Prepare 阶段下图演示了两个 Proposer 和三个 Acceptor 的系统中运行该算法的初始过程，每个 Proposer 都会向所有 Acceptor 发送 Prepare 请求。 当 Acceptor 接收到一个 Prepare 请求，包含的提议为 [n1, v1]，并且之前还未接收过 Prepare 请求，那么发送一个 Prepare 响应，设置当前接收到的提议为 [n1, v1]，并且保证以后不会再接受序号小于 n1 的提议。 如下图，Acceptor X 在收到 [n=2, v=8] 的 Prepare 请求时，由于之前没有接收过提议，因此就发送一个 [no previous] 的 Prepare 响应，设置当前接收到的提议为 [n=2, v=8]，并且保证以后不会再接受序号小于 2 的提议。其它的 Acceptor 类似。 如果 Acceptor 接收到一个 Prepare 请求，包含的提议为 [n2, v2]，并且之前已经接收过提议 [n1, v1]。如果 n1 &gt; n2，那么就丢弃该提议请求；否则，发送 Prepare 响应，该 Prepare 响应包含之前已经接收过的提议 [n1, v1]，设置当前接收到的提议为 [n2, v2]，并且保证以后不会再接受序号小于 n2 的提议。 如下图，Acceptor Z 收到 Proposer A 发来的 [n=2, v=8] 的 Prepare 请求，由于之前已经接收过 [n=4, v=5] 的提议，并且 n &gt; 2，因此就抛弃该提议请求；Acceptor X 收到 Proposer B 发来的 [n=4, v=5] 的 Prepare 请求，因为之前接收到的提议为 [n=2, v=8]，并且 2 &lt;= 4，因此就发送 [n=2, v=8] 的 Prepare 响应，设置当前接收到的提议为 [n=4, v=5]，并且保证以后不会再接受序号小于 4 的提议。Acceptor Y 类似。 Accept 阶段当一个 Proposer 接收到超过一半 Acceptor 的 Prepare 响应时，就可以发送 Accept 请求。 Proposer A 接收到两个 Prepare 响应之后，就发送 [n=2, v=8] Accept 请求。该 Accept 请求会被所有 Acceptor 丢弃，因为此时所有 Acceptor 都保证不接受序号小于 4 的提议。 Proposer B 过后也收到了两个 Prepare 响应，因此也开始发送 Accept 请求。需要注意的是，Accept 请求的 v 需要取它收到的最大提议编号对应的 v 值，也就是 8。因此它发送 [n=4, v=8] 的 Accept 请求。 Learn 阶段Acceptor 接收到 Accept 请求时，如果序号大于等于该 Acceptor 承诺的最小序号，那么就发送 Learn 提议给所有的 Learner。当 Learner 发现有大多数的 Acceptor 接收了某个提议，那么该提议的提议值就被 Paxos 选择出来。 Raft算法 https://github.com/CyC2018/CS-Notes/blob/master/notes/%E5%88%86%E5%B8%83%E5%BC%8F.md#%E4%BA%94paxos]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch简介]]></title>
    <url>%2F2019%2F07%2F01%2FElaticSearch%2F</url>
    <content type="text"><![CDATA[ElasticSearch基本概念 节点(node): 一个节点是一个逻辑上独立的服务，可以存储数据，并参与集群的索引和搜索功能, 一个节点也有唯一的名字，群集通过节点名称进行管理和通信. 索引（Index) ： 索引与关系型数据库实例(Database)相当。索引只是一个 逻辑命名空间，它指向一个或多个分片(shards)，内部用Apache Lucene实现索引中数据的读写 文档类型（Type）：相当于数据库中的table概念。每个文档在ElasticSearch中都必须设定它的类型。文档类型使得同一个索引中在存储结构不同文档时，只需要依据文档类型就可以找到对应的参数映射(Mapping)信息，方便文档的存取 文档（Document) ：相当于数据库中的row， 是可以被索引的基本单位。文档是以JSON格式存储的。在一个索引中，您可以存储多个的文档。请注意，虽然在一个索引中有多分文档，但这些文档的结构是一致的，并在第一次存储的时候指定, 文档属于一种 类型(type)，各种各样的类型存在于一个索引中。 集群（Cluster): 包含一个或多个具有相同 cluster.name 的节点. 集群内节点协同工作，共享数据，并共同分担工作负荷。 由于节点是从属集群的，集群会自我重组来均匀地分发数据. cluster Name是很重要的，因为每个节点只能是群集的一部分，当该节点被设置为相同的名称时，就会自动加入群集。 集群中通过选举产生一个mater节点，它将负责管理集群范畴的变更，例如创建或删除索引，添加节点到集群或从集群删除节点。master 节点无需参与文档层面的变更和搜索，这意味着仅有一个 master 节点并不会因流量增长而成为瓶颈。任意一个节点都可以成为 master 节点。我们例举的集群只有一个节点，因此它会扮演 master 节点的角色。 作为用户，我们可以访问包括 master 节点在内的集群中的任一节点。每个节点都知道各个文档的位置，并能够将我们的请求直接转发到拥有我们想要的数据的节点。无论我们访问的是哪个节点，它都会控制从拥有数据的节点收集响应的过程，并返回给客户端最终的结果。这一切都是由 Elasticsearch 透明管理的 分片(shard) ：是 工作单元(worker unit) 底层的一员，用来分配集群中的数据，它只负责保存索引中所有数据的一小片。 分片是一个独立的Lucene实例，并且它自身也是一个完整的搜索引擎。 文档存储并且被索引在分片中，但是我们的程序并不会直接与它们通信。取而代之，它们直接与索引进行通信的 把分片想象成一个数据的容器。数据被存储在分片中，然后分片又被分配在集群的节点上。当你的集群扩展或者缩小时，elasticsearch 会自动的在节点之间迁移分配分片，以便集群保持均衡 分片分为 主分片(primary shard) 以及 从分片(replica shard) 两种。在你的索引中，每一个文档都属于一个主分片 从分片只是主分片的一个副本，它用于提供数据的冗余副本，在硬件故障时提供数据保护，同时服务于搜索和检索这种只读请求 索引中的主分片的数量在索引创建后就固定下来了，但是从分片的数量可以随时改变。 一个索引默认设置了5个主分片，每个主分片有一个从分片对应 副本（Replica）：同一个分片(Shard)的备份数据，一个分片可能会有0个或多个副本，这些副本中的数据保证强一致或最终一致。 Elasticsearch集群 Elasticsearch集群搜索（全文转载自 Elasticsearch内核解析 - 查询篇） 目前的Elasticsearch有两个明显的身份，一个是分布式搜索系统，另一个是分布式NoSQL数据库，对于这两种不同的身份，读写语义基本类似，但也有一点差异。 读操作实时性对于搜索而言是近实时的，延迟在100ms以上，对于NoSQL则需要是实时的。 一致性指的是写入成功后，下次读操作一定要能读取到最新的数据。对于搜索，这个要求会低一些，可以有一些延迟。但是对于NoSQL数据库，则一般要求最好是强一致性的。 结果匹配上，NoSQL作为数据库，查询过程中只有符合不符合两种情况，而搜索里面还有是否相关，类似于NoSQL的结果只能是0或1，而搜索里面可能会有0.1，0.5，0.9等部分匹配或者更相关的情况。 结果召回上，搜索一般只需要召回最满足条件的Top N结果即可，而NoSQL一般都需要返回满足条件的所有结果。 搜索系统一般都是两阶段查询，第一个阶段查询到对应的Doc ID，也就是PK；第二阶段再通过Doc ID去查询完整文档，而NoSQL数据库一般是一阶段就返回结果。在Elasticsearch中两种都支持。 目前NoSQL的查询，聚合、分析和统计等功能上都是要比搜索弱的。 Lucene的读Elasticsearch使用了Lucene作为搜索引擎库，通过Lucene完成特定字段的搜索等功能，在Lucene中这个功能是通过IndexSearcher的下列接口实现的： public TopDocs search(Query query, int n); public Document doc(int docID); public int count(Query query); ......(其他) 第一个search接口实现搜索功能，返回最满足Query的N个结果；第二个doc接口通过doc id查询Doc内容；第三个count接口通过Query获取到命中数。 这三个功能是搜索中的最基本的三个功能点，对于大部分Elasticsearch中的查询都是比较复杂的，直接用这个接口是无法满足需求的，比如分布式问题。这些问题都留给了Elasticsearch解决，我们接下来看Elasticsearch中相关读功能的剖析。 Elasticsearch的读Elasticsearch中每个Shard都会有多个Replica，主要是为了保证数据可靠性，除此之外，还可以增加读能力，因为写的时候虽然要写大部分Replica Shard，但是查询的时候只需要查询Primary和Replica中的任何一个就可以了。 在上图中，该Shard有1个Primary和2个Replica Node，当查询的时候，从三个节点中根据Request中的preference参数选择一个节点查询。preference可以设置_local，_primary，_replica以及其他选项。如果选择了primary，则每次查询都是直接查询Primary，可以保证每次查询都是最新的。如果设置了其他参数，那么可能会查询到R1或者R2，这时候就有可能查询不到最新的数据。 接下来看一下，Elasticsearch中的查询是如何支持分布式的。 Elasticsearch中通过分区实现分布式，数据写入的时候根据_routing规则将数据写入某一个Shard中，这样就能将海量数据分布在多个Shard以及多台机器上，已达到分布式的目标。这样就导致了查询的时候，潜在数据会在当前index的所有的Shard中，所以Elasticsearch查询的时候需要查询所有Shard，同一个Shard的Primary和Replica选择一个即可，查询请求会分发给所有Shard，每个Shard中都是一个独立的查询引擎，比如需要返回Top 10的结果，那么每个Shard都会查询并且返回Top 10的结果，然后在Client Node里面会接收所有Shard的结果，然后通过优先级队列二次排序，选择出Top 10的结果返回给用户。 这里有一个问题就是请求膨胀，用户的一个搜索请求在Elasticsearch内部会变成Shard个请求，这里有个优化点，虽然是Shard个请求，但是这个Shard个数不一定要是当前Index中的Shard个数，只要是当前查询相关的Shard即可，这个需要基于业务和请求内容优化，通过这种方式可以优化请求膨胀数。 Elasticsearch中的查询主要分为两类，Get请求：通过ID查询特定Doc；Search请求：通过Query查询匹配Doc。 上图中内存中的Segment是指刚Refresh Segment，但是还没持久化到磁盘的新Segment，而非从磁盘加载到内存中的Segment。 对于Search类请求，查询的时候是一起查询内存和磁盘上的Segment，最后将结果合并后返回。这种查询是近实时（Near Real Time）的，主要是由于内存中的Index数据需要一段时间后才会刷新为Segment。 对于Get类请求，查询的时候是先查询内存中的TransLog，如果找到就立即返回，如果没找到再查询磁盘上的TransLog，如果还没有则再去查询磁盘上的Segment。这种查询是实时（Real Time）的。这种查询顺序可以保证查询到的Doc是最新版本的Doc，这个功能也是为了保证NoSQL场景下的实时性要求。 所有的搜索系统一般都是两阶段查询，第一阶段查询到匹配的DocID，第二阶段再查询DocID对应的完整文档，这种在Elasticsearch中称为query_then_fetch，还有一种是一阶段查询的时候就返回完整Doc，在Elasticsearch中称作query_and_fetch，一般第二种适用于只需要查询一个Shard的请求。 除了一阶段，两阶段外，还有一种三阶段查询的情况。搜索里面有一种算分逻辑是根据TF（Term Frequency）和DF（Document Frequency）计算基础分，但是Elasticsearch中查询的时候，是在每个Shard中独立查询的，每个Shard中的TF和DF也是独立的，虽然在写入的时候通过_routing保证Doc分布均匀，但是没法保证TF和DF均匀，那么就有会导致局部的TF和DF不准的情况出现，这个时候基于TF、DF的算分就不准。为了解决这个问题，Elasticsearch中引入了DFS查询，比如DFS_query_then_fetch，会先收集所有Shard中的TF和DF值，然后将这些值带入请求中，再次执行query_then_fetch，这样算分的时候TF和DF就是准确的，类似的有DFS_query_and_fetch。这种查询的优势是算分更加精准，但是效率会变差。另一种选择是用BM25代替TF/DF模型。 在新版本Elasticsearch中，用户没法指定DFS_query_and_fetch和query_and_fetch，这两种只能被Elasticsearch系统改写。 Elasticsearch查询流程Elasticsearch中的大部分查询，以及核心功能都是Search类型查询，上面我们了解到查询分为一阶段，二阶段和三阶段，这里我们就以最常见的的二阶段查询为例来介绍查询流程。 注册ActionElasticsearch中，查询和写操作一样都是在ActionModule.java中注册入口处理函数的。 registerHandler.accept(new RestSearchAction(settings, restController)); ...... actions.register(SearchAction.INSTANCE, TransportSearchAction.class); ...... 如果请求是Rest请求，则会在RestSearchAction中解析请求，检查查询类型，不能设置为dfs_query_and_fetch或者query_and_fetch，这两个目前只能用于Elasticsearch中的优化场景，然后将请求发给后面的TransportSearchAction处理。然后构造SearchRequest，将请求发送给TransportSearchAction处理。 如果是第一阶段的Query Phase请求，则会调用SearchService的executeQueryPhase方法。 如果是第二阶段的Fetch Phase请求，则会调用SearchService的executeFetchPhase方法。 Client NodeClient Node 也包括了前面说过的Parse Request，这里就不再赘述了，接下来看一下其他的部分。 Get Remove Cluster Shard 判断是否需要跨集群访问，如果需要，则获取到要访问的Shard列表。 Get Search Shard Iterator 获取当前Cluster中要访问的Shard，和上一步中的Remove Cluster Shard合并，构建出最终要访问的完整Shard列表。 这一步中，会根据Request请求中的参数从Primary Node和多个Replica Node中选择出一个要访问的Shard。 For Every Shard:Perform 遍历每个Shard，对每个Shard执行后面逻辑。 Send Request To Query Shard 将查询阶段请求发送给相应的Shard。 Merge Docs 上一步将请求发送给多个Shard后，这一步就是异步等待返回结果，然后对结果合并。这里的合并策略是维护一个Top N大小的优先级队列，每当收到一个shard的返回，就把结果放入优先级队列做一次排序，直到所有的Shard都返回。 翻页逻辑也是在这里，如果需要取Top 30~ Top 40的结果，这个的意思是所有Shard查询结果中的第30到40的结果，那么在每个Shard中无法确定最终的结果，每个Shard需要返回Top 40的结果给Client Node，然后Client Node中在merge docs的时候，计算出Top 40的结果，最后再去除掉Top 30，剩余的10个结果就是需要的Top 30~ Top 40的结果。 上述翻页逻辑有一个明显的缺点就是每次Shard返回的数据中包括了已经翻过的历史结果，如果翻页很深，则在这里需要排序的Docs会很多，比如Shard有1000，取第9990到10000的结果，那么这次查询，Shard总共需要返回1000 * 10000，也就是一千万Doc，这种情况很容易导致OOM。 另一种翻页方式是使用search_after，这种方式会更轻量级，如果每次只需要返回10条结构，则每个Shard只需要返回search_after之后的10个结果即可，返回的总数据量只是和Shard个数以及本次需要的个数有关，和历史已读取的个数无关。这种方式更安全一些，推荐使用这种。 如果有aggregate，也会在这里做聚合，但是不同的aggregate类型的merge策略不一样，具体的可以在后面的aggregate文章中再介绍。 Send Request To Fetch Shard 选出Top N个Doc ID后发送给这些Doc ID所在的Shard执行Fetch Phase，最后会返回Top N的Doc的内容。 Query Phase接下来我们看第一阶段查询的步骤： Create Search Context 创建Search Context，之后Search过程中的所有中间状态都会存在Context中，这些状态总共有50多个，具体可以查看DefaultSearchContext或者其他SearchContext的子类。 Parse Query 解析Query的Source，将结果存入Search Context。这里会根据请求中Query类型的不同创建不同的Query对象，比如TermQuery、FuzzyQuery等，最终真正执行TermQuery、FuzzyQuery等语义的地方是在Lucene中。 这里包括了dfsPhase、queryPhase和fetchPhase三个阶段的preProcess部分，只有queryPhase的preProcess中有执行逻辑，其他两个都是空逻辑，执行完preProcess后，所有需要的参数都会设置完成。 由于Elasticsearch中有些请求之间是相互关联的，并非独立的，比如scroll请求，所以这里同时会设置Context的生命周期。 同时会设置lowLevelCancellation是否打开，这个参数是集群级别配置，同时也能动态开关，打开后会在后面执行时做更多的检测，检测是否需要停止后续逻辑直接返回。 Get From Cache 判断请求是否允许被Cache，如果允许，则检查Cache中是否已经有结果，如果有则直接读取Cache，如果没有则继续执行后续步骤，执行完后，再将结果加入Cache。 Add Collectors Collector主要目标是收集查询结果，实现排序，对自定义结果集过滤和收集等。这一步会增加多个Collectors，多个Collector组成一个List。 FilteredCollector：先判断请求中是否有Post Filter，Post Filter用于Search，Agg等结束后再次对结果做Filter，希望Filter不影响Agg结果。如果有Post Filter则创建一个FilteredCollector，加入Collector List中。 PluginInMultiCollector：判断请求中是否制定了自定义的一些Collector，如果有，则创建后加入Collector List。 MinimumScoreCollector：判断请求中是否制定了最小分数阈值，如果指定了，则创建MinimumScoreCollector加入Collector List中，在后续收集结果时，会过滤掉得分小于最小分数的Doc。 EarlyTerminatingCollector：判断请求中是否提前结束Doc的Seek，如果是则创建EarlyTerminatingCollector，加入Collector List中。在后续Seek和收集Doc的过程中，当Seek的Doc数达到Early Terminating后会停止Seek后续倒排链。 CancellableCollector：判断当前操作是否可以被中断结束，比如是否已经超时等，如果是会抛出一个TaskCancelledException异常。该功能一般用来提前结束较长的查询请求，可以用来保护系统。 EarlyTerminatingSortingCollector：如果Index是排序的，那么可以提前结束对倒排链的Seek，相当于在一个排序递减链表上返回最大的N个值，只需要直接返回前N个值就可以了。这个Collector会加到Collector List的头部。EarlyTerminatingSorting和EarlyTerminating的区别是，EarlyTerminatingSorting是一种对结果无损伤的优化，而EarlyTerminating是有损的，人为掐断执行的优化。 TopDocsCollector：这个是最核心的Top N结果选择器，会加入到Collector List的头部。TopScoreDocCollector和TopFieldCollector都是TopDocsCollector的子类，TopScoreDocCollector会按照固定的方式算分，排序会按照分数+doc id的方式排列，如果多个doc的分数一样，先选择doc id小的文档。而TopFieldCollector则是根据用户指定的Field的值排序。 lucene::search 这一步会调用Lucene中IndexSearch的search接口，执行真正的搜索逻辑。每个Shard中会有多个Segment，每个Segment对应一个LeafReaderContext，这里会遍历每个Segment，到每个Segment中去Search结果，然后计算分数。 搜索里面一般有两阶段算分，第一阶段是在这里算的，会对每个Seek到的Doc都计算分数，为了减少CPU消耗，一般是算一个基本分数。这一阶段完成后，会有个排序。然后在第二阶段，再对Top 的结果做一次二阶段算分，在二阶段算分的时候会考虑更多的因子。二阶段算分在后续操作中。 具体请求，比如TermQuery、WildcardQuery的查询逻辑都在Lucene中，后面会有专门文章介绍。 rescore 根据Request中是否包含rescore配置决定是否进行二阶段排序，如果有则执行二阶段算分逻辑，会考虑更多的算分因子。二阶段算分也是一种计算机中常见的多层设计，是一种资源消耗和效率的折中。 Elasticsearch中支持配置多个Rescore，这些rescore逻辑会顺序遍历执行。每个rescore内部会先按照请求参数window选择出Top window的doc，然后对这些doc排序，排完后再合并回原有的Top 结果顺序中。 suggest::execute() 如果有推荐请求，则在这里执行推荐请求。如果请求中只包含了推荐的部分，则很多地方可以优化。推荐不是今天的重点，这里就不介绍了，后面有机会再介绍。 aggregation::execute() 如果含有聚合统计请求，则在这里执行。Elasticsearch中的aggregate的处理逻辑也类似于Search，通过多个Collector来实现。在Client Node中也需要对aggregation做合并。aggregate逻辑更复杂一些，就不在这里赘述了，后面有需要就再单独开文章介绍。 上述逻辑都执行完成后，如果当前查询请求只需要查询一个Shard，那么会直接在当前Node执行Fetch Phase。 Fetch PhaseElasticsearch作为搜索系统时，或者任何搜索系统中，除了Query阶段外，还会有一个Fetch阶段，这个Fetch阶段在数据库类系统中是没有的，是搜索系统中额外增加的阶段。搜索系统中额外增加Fetch阶段的原因是搜索系统中数据分布导致的，在搜索中，数据通过routing分Shard的时候，只能根据一个主字段值来决定，但是查询的时候可能会根据其他非主字段查询，那么这个时候所有Shard中都可能会存在相同非主字段值的Doc，所以需要查询所有Shard才能不会出现结果遗漏。同时如果查询主字段，那么这个时候就能直接定位到Shard，就只需要查询特定Shard即可，这个时候就类似于数据库系统了。另外，数据库中的二级索引又是另外一种情况，但类似于查主字段的情况，这里就不多说了。 基于上述原因，第一阶段查询的时候并不知道最终结果会在哪个Shard上，所以每个Shard中管都需要查询完整结果，比如需要Top 10，那么每个Shard都需要查询当前Shard的所有数据，找出当前Shard的Top 10，然后返回给Client Node。如果有100个Shard，那么就需要返回100 * 10 = 1000个结果，而Fetch Doc内容的操作比较耗费IO和CPU，如果在第一阶段就Fetch Doc，那么这个资源开销就会非常大。所以，一般是当Client Node选择出最终Top N的结果后，再对最终的Top N读取Doc内容。通过增加一点网络开销而避免大量IO和CPU操作，这个折中是非常划算的。 Fetch阶段的目的是通过DocID获取到用户需要的完整Doc内容。这些内容包括了DocValues，Store，Source，Script和Highlight等，具体的功能点是在SearchModule中注册的，系统默认注册的有： ExplainFetchSubPhase DocValueFieldsFetchSubPhase ScriptFieldsFetchSubPhase FetchSourceSubPhase VersionFetchSubPhase MatchedQueriesFetchSubPhase HighlightPhase ParentFieldSubFetchPhase 除了系统默认的8种外，还有通过插件的形式注册自定义的功能，这些SubPhase中最重要的是Source和Highlight，Source是加载原文，Highlight是计算高亮显示的内容片断。 上述多个SubPhase会针对每个Doc顺序执行，可能会产生多次的随机IO，这里会有一些优化方案，但是都是针对特定场景的，不具有通用性。 Fetch Phase执行完后，整个查询流程就结束了。 Elasticsearch集群写入（全文转载自 Elasticsearch内核解析 - 写入篇） 写操作 实时性： 搜索系统的Index一般都是NRT（Near Real Time），近实时的，比如Elasticsearch中，Index的实时性是由refresh控制的，默认是1s，最快可到100ms，那么也就意味着Index doc成功后，需要等待一秒钟后才可以被搜索到。 NoSQL数据库的Write基本都是RT（Real Time），实时的，写入成功后，立即是可见的。Elasticsearch中的Index请求也能保证是实时的，因为Get请求会直接读内存中尚未Flush到存储介质的TransLog。 可靠性： 搜索系统对可靠性要求都不高，一般数据的可靠性通过将原始数据存储在另一个存储系统来保证，当搜索系统的数据发生丢失时，再从其他存储系统导一份数据过来重新rebuild就可以了。在Elasticsearch中，通过设置TransLog的Flush频率可以控制可靠性，要么是按请求，每次请求都Flush；要么是按时间，每隔一段时间Flush一次。一般为了性能考虑，会设置为每隔5秒或者1分钟Flush一次，Flush间隔时间越长，可靠性就会越低。 NoSQL数据库作为一款数据库，必须要有很高的可靠性，数据可靠性是生命底线，决不能有闪失。如果把Elasticsearch当做NoSQL数据库，此时需要设置TransLog的Flush策略为每个请求都要Flush，这样才能保证当前Shard写入成功后，数据能尽量持久化下来。 写操作的关键点在考虑或分析一个分布式系统的写操作时，一般需要从下面几个方面考虑： 可靠性：或者是持久性，数据写入系统成功后，数据不会被回滚或丢失。 一致性：数据写入成功后，再次查询时必须能保证读取到最新版本的数据，不能读取到旧数据。 原子性：一个写入或者更新操作，要么完全成功，要么完全失败，不允许出现中间状态。 隔离性：多个写入操作相互不影响。 实时性：写入后是否可以立即被查询到。 性能：写入性能，吞吐量到底怎么样。 Elasticsearch作为分布式系统，也需要在写入的时候满足上述的四个特点，我们在后面的写流程介绍中会涉及到上述四个方面。 接下来,我们一层一层剖析Elasticsearch内部的写机制。 Lucene的写众所周知，Elasticsearch内部使用了Lucene完成索引创建和搜索功能，Lucene中写操作主要是通过IndexWriter类实现，IndexWriter提供三个接口： public long addDocument(); public long updateDocuments(); public long deleteDocuments(); 通过这三个接口可以完成单个文档的写入，更新和删除功能，包括了分词，倒排创建，正排创建等等所有搜索相关的流程。只要Doc通过IndesWriter写入后，后面就可以通过IndexSearcher搜索了，看起来功能已经完善了，但是仍然有一些问题没有解： 上述操作是单机的，而不是我们需要的分布式。 文档写入Lucene后并不是立即可查询的，需要生成完整的Segment后才可被搜索，如何保证实时性？ Lucene生成的Segment是在内存中，如果机器宕机或掉电后，内存中的Segment会丢失，如何保证数据可靠性 ？ Lucene不支持部分文档更新，但是这又是一个强需求，如何支持部分更新？ 上述问题，在Lucene中是没有解决的，那么就需要Elasticsearch中解决上述问题。 Elasticsearch的写Elasticsearch采用多Shard方式，通过配置routing规则将数据分成多个数据子集，每个数据子集提供独立的索引和搜索功能。当写入文档的时候，根据routing规则，将文档发送给特定Shard中建立索引。这样就能实现分布式了。 此外，Elasticsearch整体架构上采用了一主多副的方式： 每个Index由多个Shard组成，每个Shard有一个主节点和多个副本节点，副本个数可配。但每次写入的时候，写入请求会先根据_routing规则选择发给哪个Shard，Index Request中可以设置使用哪个Filed的值作为路由参数，如果没有设置，则使用Mapping中的配置，如果mapping中也没有配置，则使用_id作为路由参数，然后通过_routing的Hash值选择出Shard（在OperationRouting类中），最后从集群的Meta中找出出该Shard的Primary节点。 请求接着会发送给Primary Shard，在Primary Shard上执行成功后，再从Primary Shard上将请求同时发送给多个Replica Shard，请求在多个Replica Shard上执行成功并返回给Primary Shard后，写入请求执行成功，返回结果给客户端。 这种模式下，写入操作的延时就等于latency = Latency(Primary Write) + Max(Replicas Write)。只要有副本在，写入延时最小也是两次单Shard的写入时延总和，写入效率会较低，但是这样的好处也很明显，避免写入后，单机或磁盘故障导致数据丢失，在数据重要性和性能方面，一般都是优先选择数据，除非一些允许丢数据的特殊场景。 采用多个副本后，避免了单机或磁盘故障发生时，对已经持久化后的数据造成损害，但是Elasticsearch里为了减少磁盘IO保证读写性能，一般是每隔一段时间（比如5分钟）才会把Lucene的Segment写入磁盘持久化，对于写入内存，但还未Flush到磁盘的Lucene数据，如果发生机器宕机或者掉电，那么内存中的数据也会丢失，这时候如何保证？ 对于这种问题，Elasticsearch学习了数据库中的处理方式：增加CommitLog模块，Elasticsearch中叫TransLog。 在每一个Shard中，写入流程分为两部分，先写入Lucene，再写入TransLog。 写入请求到达Shard后，先写Lucene文件，创建好索引，此时索引还在内存里面，接着去写TransLog，写完TransLog后，刷新TransLog数据到磁盘上，写磁盘成功后，请求返回给用户。这里有几个关键点，一是和数据库不同，数据库是先写CommitLog，然后再写内存，而Elasticsearch是先写内存，最后才写TransLog，一种可能的原因是Lucene的内存写入会有很复杂的逻辑，很容易失败，比如分词，字段长度超过限制等，比较重，为了避免TransLog中有大量无效记录，减少recover的复杂度和提高速度，所以就把写Lucene放在了最前面。二是写Lucene内存后，并不是可被搜索的，需要通过Refresh把内存的对象转成完整的Segment后，然后再次reopen后才能被搜索，一般这个时间设置为1秒钟，导致写入Elasticsearch的文档，最快要1秒钟才可被从搜索到，所以Elasticsearch在搜索方面是NRT（Near Real Time）近实时的系统。三是当Elasticsearch作为NoSQL数据库时，查询方式是GetById，这种查询可以直接从TransLog中查询，这时候就成了RT（Real Time）实时系统。四是每隔一段比较长的时间，比如30分钟后，Lucene会把内存中生成的新Segment刷新到磁盘上，刷新后索引文件已经持久化了，历史的TransLog就没用了，会清空掉旧的TransLog。 上面介绍了Elasticsearch在写入时的两个关键模块，Replica和TransLog，接下来，我们看一下Update流程： Lucene中不支持部分字段的Update，所以需要在Elasticsearch中实现该功能，具体流程如下： 收到Update请求后，从Segment或者TransLog中读取同id的完整Doc，记录版本号为V1。 将版本V1的全量Doc和请求中的部分字段Doc合并为一个完整的Doc，同时更新内存中的VersionMap。获取到完整Doc后，Update请求就变成了Index请求。 加锁。 再次从versionMap中读取该id的最大版本号V2，如果versionMap中没有，则从Segment或者TransLog中读取，这里基本都会从versionMap中获取到。 检查版本是否冲突(V1==V2)，如果冲突，则回退到开始的“Update doc”阶段，重新执行。如果不冲突，则执行最新的Add请求。 在Index Doc阶段，首先将Version + 1得到V3，再将Doc加入到Lucene中去，Lucene中会先删同id下的已存在doc id，然后再增加新Doc。写入Lucene成功后，将当前V3更新到versionMap中。 释放锁，部分更新的流程就结束了。 介绍完部分更新的流程后，大家应该从整体架构上对Elasticsearch的写入有了一个初步的映象，接下来我们详细剖析下写入的详细步骤。 Elasticsearch写入请求类型Elasticsearch中的写入请求类型，主要包括下列几个：Index(Create)，Update，Delete和Bulk，其中前3个是单文档操作，后一个Bulk是多文档操作，其中Bulk中可以包括Index(Create)，Update和Delete。 在6.0.0及其之后的版本中，前3个单文档操作的实现基本都和Bulk操作一致，甚至有些就是通过调用Bulk的接口实现的。估计接下来几个版本后，Index(Create)，Update，Delete都会被当做Bulk的一种特例化操作被处理。这样，代码和逻辑都会更清晰一些。 下面，我们就以Bulk请求为例来介绍写入流程。 Elasticsearch写入流程图 红色：Client Node。 绿色：Primary Node。 蓝色：Replica Node。 注册Action在Elasticsearch中，所有action的入口处理方法都是注册在ActionModule.java中，比如Bulk Request有两个注册入口，分别是Rest和Transport入口. 如果请求是Rest请求，则会在RestBulkAction中Parse Request，构造出BulkRequest，然后发给后面的TransportAction处理。 TransportShardBulkAction的基类TransportReplicationAction中注册了对Primary，Replica等的不同处理入口: 这里对原始请求，Primary Node请求和Replica Node请求各自注册了一个handler处理入口。 Client NodeClient Node 也包括了前面说过的Parse Request，这里就不再赘述了，接下来看一下其他的部分。 Ingest Pipeline 在这一步可以对原始文档做一些处理，比如HTML解析，自定义的处理，具体处理逻辑可以通过插件来实现。在Elasticsearch中，由于Ingest Pipeline会比较耗费CPU等资源，可以设置专门的Ingest Node，专门用来处理Ingest Pipeline逻辑。 如果当前Node不能执行Ingest Pipeline，则会将请求发给另一台可以执行Ingest Pipeline的Node。 Auto Create Index 判断当前Index是否存在，如果不存在，则需要自动创建Index，这里需要和Master交互。也可以通过配置关闭自动创建Index的功能。 Set Routing 设置路由条件，如果Request中指定了路由条件，则直接使用Request中的Routing，否则使用Mapping中配置的，如果Mapping中无配置，则使用默认的_id字段值。 在这一步中，如果没有指定id字段，则会自动生成一个唯一的_id字段，目前使用的是UUID。 Construct BulkShardRequest 由于Bulk Request中会包括多个(Index/Update/Delete)请求，这些请求根据routing可能会落在多个Shard上执行，这一步会按Shard挑拣Single Write Request，同一个Shard中的请求聚集在一起，构建BulkShardRequest，每个BulkShardRequest对应一个Shard。 Send Request To Primary 这一步会将每一个BulkShardRequest请求发送给相应Shard的Primary Node。 Primary NodePrimary 请求的入口是在PrimaryOperationTransportHandler的messageReceived，我们来看一下相关的逻辑流程。 Index or Update or Delete 循环执行每个Single Write Request，对于每个Request，根据操作类型(CREATE/INDEX/UPDATE/DELETE)选择不同的处理逻辑。 其中，Create/Index是直接新增Doc，Delete是直接根据_id删除Doc，Update会稍微复杂些，我们下面就以Update为例来介绍。 Translate Update To Index or Delete 这一步是Update操作的特有步骤，在这里，会将Update请求转换为Index或者Delete请求。首先，会通过GetRequest查询到已经存在的同_id Doc（如果有）的完整字段和值（依赖_source字段），然后和请求中的Doc合并。同时，这里会获取到读到的Doc版本号，记做V1。 Parse Doc 这里会解析Doc中各个字段。生成ParsedDocument对象，同时会生成uid Term。在Elasticsearch中，_uid = type # _id，对用户，_Id可见，而Elasticsearch中存储的是_uid。这一部分生成的ParsedDocument中也有Elasticsearch的系统字段，大部分会根据当前内容填充，部分未知的会在后面继续填充ParsedDocument。 Update Mapping Elasticsearch中有个自动更新Mapping的功能，就在这一步生效。会先挑选出Mapping中未包含的新Field，然后判断是否运行自动更新Mapping，如果允许，则更新Mapping。 Get Sequence Id and Version 由于当前是Primary Shard，则会从SequenceNumber Service获取一个sequenceID和Version。SequenceID在Shard级别每次递增1，SequenceID在写入Doc成功后，会用来初始化LocalCheckpoint。Version则是根据当前Doc的最大Version递增1。 Add Doc To Lucene 这一步开始的时候会给特定_uid加锁，然后判断该_uid对应的Version是否等于之前Translate Update To Index步骤里获取到的Version，如果不相等，则说明刚才读取Doc后，该Doc发生了变化，出现了版本冲突，这时候会抛出一个VersionConflict的异常，该异常会在Primary Node最开始处捕获，重新从“Translate Update To Index or Delete”开始执行。 如果Version相等，则继续执行，如果已经存在同id的Doc，则会调用Lucene的UpdateDocument(uid, doc)接口，先根据uid删除Doc，然后再Index新Doc。如果是首次写入，则直接调用Lucene的AddDocument接口完成Doc的Index，AddDocument也是通过UpdateDocument实现。 这一步中有个问题是，如何保证Delete-Then-Add的原子性，怎么避免中间状态时被Refresh？答案是在开始Delete之前，会加一个Refresh Lock，禁止被Refresh，只有等Add完后释放了Refresh Lock后才能被Refresh，这样就保证了Delete-Then-Add的原子性。 Lucene的UpdateDocument接口中就只是处理多个Field，会遍历每个Field逐个处理，处理顺序是invert index，store field，doc values，point dimension，后续会有文章专门介绍Lucene中的写入。 Write Translog 写完Lucene的Segment后，会以keyvalue的形式写TransLog，Key是_id，Value是Doc内容。当查询的时候，如果请求是GetDocByID，则可以直接根据_id从TransLog中读取到，满足NoSQL场景下的实时性要去。 需要注意的是，这里只是写入到内存的TransLog，是否Sync到磁盘的逻辑还在后面。 这一步的最后，会标记当前SequenceID已经成功执行，接着会更新当前Shard的LocalCheckPoint。 Renew Bulk Request 这里会重新构造Bulk Request，原因是前面已经将UpdateRequest翻译成了Index或Delete请求，则后续所有Replica中只需要执行Index或Delete请求就可以了，不需要再执行Update逻辑，一是保证Replica中逻辑更简单，性能更好，二是保证同一个请求在Primary和Replica中的执行结果一样。 Flush Translog 这里会根据TransLog的策略，选择不同的执行方式，要么是立即Flush到磁盘，要么是等到以后再Flush。Flush的频率越高，可靠性越高，对写入性能影响越大。 Send Requests To Replicas 这里会将刚才构造的新的Bulk Request并行发送给多个Replica，然后等待Replica的返回，这里需要等待所有Replica返回后（可能有成功，也有可能失败），Primary Node才会返回用户。如果某个Replica失败了，则Primary会给Master发送一个Remove Shard请求，要求Master将该Replica Shard从可用节点中移除。 这里，同时会将SequenceID，PrimaryTerm，GlobalCheckPoint等传递给Replica。 发送给Replica的请求中，Action Name等于原始ActionName + [R]，这里的R表示Replica。通过这个[R]的不同，可以找到处理Replica请求的Handler。 Receive Response From Replicas Replica中请求都处理完后，会更新Primary Node的LocalCheckPoint。 Replica NodeReplica 请求的入口是在ReplicaOperationTransportHandler的messageReceived，我们来看一下相关的逻辑流程。 Index or Delete 根据请求类型是Index还是Delete，选择不同的执行逻辑。这里没有Update，是因为在Primary Node中已经将Update转换成了Index或Delete请求了。 Parse Doc Update Mapping 以上都和Primary Node中逻辑一致。 Get Sequence Id and Version Primary Node中会生成Sequence ID和Version，然后放入ReplicaRequest中，这里只需要从Request中获取到就行。 Add Doc To Lucene 由于已经在Primary Node中将部分Update请求转换成了Index或Delete请求，这里只需要处理Index和Delete两种请求，不再需要处理Update请求了。比Primary Node会更简单一些。 Write Translog Flush Translog 以上都和Primary Node中逻辑一致。 ES写入总结上面详细介绍了Elasticsearch的写入流程及其各个流程的工作机制，我们在这里再次总结下之前提出的分布式系统中的六大特性： 可靠性：由于Lucene的设计中不考虑可靠性，在Elasticsearch中通过Replica和TransLog两套机制保证数据的可靠性。一致性：Lucene中的Flush锁只保证Update接口里面Delete和Add中间不会Flush，但是Add完成后仍然有可能立即发生Flush，导致Segment可读。这样就没法保证Primary和所有其他Replica可以同一时间Flush，就会出现查询不稳定的情况，这里只能实现最终一致性。原子性：Add和Delete都是直接调用Lucene的接口，是原子的。当部分更新时，使用Version和锁保证更新是原子的。隔离性：仍然采用Version和局部锁来保证更新的是特定版本的数据。实时性：使用定期Refresh Segment到内存，并且Reopen Segment方式保证搜索可以在较短时间（比如1秒）内被搜索到。通过将未刷新到磁盘数据记入TransLog，保证对未提交数据可以通过ID实时访问到。性能：性能是一个系统性工程，所有环节都要考虑对性能的影响，在Elasticsearch中，在很多地方的设计都考虑到了性能，一是不需要所有Replica都返回后才能返回给用户，只需要返回特定数目的就行；二是生成的Segment现在内存中提供服务，等一段时间后才刷新到磁盘，Segment在内存这段时间的可靠性由TransLog保证；三是TransLog可以配置为周期性的Flush，但这个会给可靠性带来伤害；四是每个线程持有一个Segment，多线程时相互不影响，相互独立，性能更好；五是系统的写入流程对版本依赖较重，读取频率较高，因此采用了versionMap，减少热点数据的多次磁盘IO开销。Lucene中针对性能做了大量的优化。后面我们也会有文章专门介绍Lucene中的优化思路。 Elasticsearch分布式一致性原理剖析-节点（全文转载自https://zhuanlan.zhihu.com/p/34858035） Elasticsearch分布式一致性原理剖析-节点目录Elasticsearch分布式一致性原理剖析”系列将会对Elasticsearch的分布式一致性原理进行详细的剖析，介绍其实现方式、原理以及其存在的问题等(基于6.2版本)。 ES目前是最流行的分布式搜索引擎系统，其使用Lucene作为单机存储引擎并提供强大的搜索查询能力。学习其搜索原理，则必须了解Lucene，而学习ES的架构，就必须了解其分布式如何实现，而一致性是分布式系统的核心之一。 本篇将介绍ES的集群组成、节点发现与Master选举，错误检测与扩缩容相关的内容。ES在处理节点发现与Master选举等方面没有选择Zookeeper等外部组件，而是自己实现的一套，本文会介绍ES的这套机制是如何工作的，存在什么问题。本文的主要内容如下： ES集群构成 节点发现 Master选举 错误检测 集群扩缩容 与Zookeeper、raft等实现方式的比较 ES集群构成首先，一个Elasticsearch集群(下面简称ES集群)是由许多节点(Node)构成的，Node可以有不同的类型，通过以下配置，可以产生四种不同类型的Node： conf/elasticsearch.yml: node.master: true/false node.data: true/false 四种不同类型的Node是一个node.master和node.data的true/false的两两组合。当然还有其他类型的Node，比如IngestNode(用于数据预处理等)，不在本文讨论范围内。 当node.master为true时，其表示这个node是一个master的候选节点，可以参与选举，在ES的文档中常被称作master-eligible node，类似于MasterCandidate。ES正常运行时只能有一个master(即leader)，多于1个时会发生脑裂。 当node.data为true时，这个节点作为一个数据节点，会存储分配在该node上的shard的数据并负责这些shard的写入、查询等。 此外，任何一个集群内的node都可以执行任何请求，其会负责将请求转发给对应的node进行处理，所以当node.master和node.data都为false时，这个节点可以作为一个类似proxy的节点，接受请求并进行转发、结果聚合等。 上图是一个ES集群的示意图，其中NodeA是当前集群的Master，NodeB和NodeC是Master的候选节点，其中NodeA和NodeB同时也是数据节点(DataNode)，此外，NodeD是一个单纯的数据节点，Node_E是一个proxy节点。每个Node会跟其他所有Node建立连接。 到这里，我们提一个问题，供读者思考：一个ES集群应当配置多少个master-eligible node，当集群的存储或者计算资源不足，需要扩容时，新扩上去的节点应该设置为何种类型？ 节点发现ZenDiscovery是ES自己实现的一套用于节点发现和选主等功能的模块，没有依赖Zookeeper等工具，官方文档： https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html 简单来说，节点发现依赖以下配置： conf/elasticsearch.yml: discovery.zen.ping.unicast.hosts: [1.1.1.1, 1.1.1.2, 1.1.1.3] 这个配置可以看作是，在本节点到每个hosts中的节点建立一条边，当整个集群所有的node形成一个联通图时，所有节点都可以知道集群中有哪些节点，不会形成孤岛。 官方推荐这里设置为所有的master-eligible node，读者可以想想这样有何好处： It is recommended that the unicast hosts list be maintained as the list of master-eligible Master选举上面提到，集群中可能会有多个master-eligible node，此时就要进行master选举，保证只有一个当选master。如果有多个node当选为master，则集群会出现脑裂，脑裂会破坏数据的一致性，导致集群行为不可控，产生各种非预期的影响。 为了避免产生脑裂，ES采用了常见的分布式系统思路，保证选举出的master被多数派(quorum)的master-eligible node认可，以此来保证只有一个master。这个quorum通过以下配置进行配置： conf/elasticsearch.yml: discovery.zen.minimum_master_nodes: 2 这个配置对于整个集群非常重要。 master选举谁发起，什么时候发起？master选举当然是由master-eligible节点发起，当一个master-eligible节点发现满足以下条件时发起选举： 该master-eligible节点的当前状态不是master。该master-eligible节点通过ZenDiscovery模块的ping操作询问其已知的集群其他节点，没有任何节点连接到master。包括本节点在内，当前已有超过minimum_master_nodes个节点没有连接到master。总结一句话，即当一个节点发现包括自己在内的多数派的master-eligible节点认为集群没有master时，就可以发起master选举。 当需要选举master时，选举谁？首先是选举谁的问题，如下面源码所示，选举的是排序后的第一个MasterCandidate(即master-eligible node)。 public MasterCandidate electMaster(Collection&lt;MasterCandidate&gt; candidates) { assert hasEnoughCandidates(candidates); List&lt;MasterCandidate&gt; sortedCandidates = new ArrayList&lt;&gt;(candidates); sortedCandidates.sort(MasterCandidate::compare); return sortedCandidates.get(0); } 那么是按照什么排序的？ public static int compare(MasterCandidate c1, MasterCandidate c2) { // we explicitly swap c1 and c2 here. the code expects &quot;better&quot; is lower in a sorted // list, so if c2 has a higher cluster state version, it needs to come first. int ret = Long.compare(c2.clusterStateVersion, c1.clusterStateVersion); if (ret == 0) { ret = compareNodes(c1.getNode(), c2.getNode()); } return ret; } 如上面源码所示，先根据节点的clusterStateVersion比较，clusterStateVersion越大，优先级越高。clusterStateVersion相同时，进入compareNodes，其内部按照节点的Id比较(Id为节点第一次启动时随机生成)。 总结一下： 当clusterStateVersion越大，优先级越高。这是为了保证新Master拥有最新的clusterState(即集群的meta)，避免已经commit的meta变更丢失。因为Master当选后，就会以这个版本的clusterState为基础进行更新。(一个例外是集群全部重启，所有节点都没有meta，需要先选出一个master，然后master再通过持久化的数据进行meta恢复，再进行meta同步)。当clusterStateVersion相同时，节点的Id越小，优先级越高。即总是倾向于选择Id小的Node，这个Id是节点第一次启动时生成的一个随机字符串。之所以这么设计，应该是为了让选举结果尽可能稳定，不要出现都想当master而选不出来的情况。 什么时候选举成功？当一个master-eligible node(我们假设为Node_A)发起一次选举时，它会按照上述排序策略选出一个它认为的master。 假设Node_A选Node_B当Master：Node_A会向Node_B发送join请求，那么此时： (1) 如果Node_B已经成为Master，Node_B就会把Node_A加入到集群中，然后发布最新的cluster_state, 最新的cluster_state就会包含Node_A的信息。相当于一次正常情况的新节点加入。对于Node_A，等新的cluster_state发布到Node_A的时候，Node_A也就完成join了。 (2) 如果Node_B在竞选Master，那么Node_B会把这次join当作一张选票。对于这种情况，Node_A会等待一段时间，看Node_B是否能成为真正的Master，直到超时或者有别的Master选成功。 (3) 如果Node_B认为自己不是Master(现在不是，将来也选不上)，那么Node_B会拒绝这次join。对于这种情况，Node_A会开启下一轮选举。 假设Node_A选自己当Master：此时NodeA会等别的node来join，即等待别的node的选票，当收集到超过半数的选票时，认为自己成为master，然后变更cluster_state中的master node为自己，并向集群发布这一消息。 有兴趣的同学可以看看下面这段源码： if (transportService.getLocalNode().equals(masterNode)) { final int requiredJoins = Math.max(0, electMaster.minimumMasterNodes() - 1); // we count as one logger.debug(&quot;elected as master, waiting for incoming joins ([{}] needed)&quot;, requiredJoins); nodeJoinController.waitToBeElectedAsMaster(requiredJoins, masterElectionWaitForJoinsTimeout, new NodeJoinController.ElectionCallback() { @Override public void onElectedAsMaster(ClusterState state) { synchronized (stateMutex) { joinThreadControl.markThreadAsDone(currentThread); } } @Override public void onFailure(Throwable t) { logger.trace(&quot;failed while waiting for nodes to join, rejoining&quot;, t); synchronized (stateMutex) { joinThreadControl.markThreadAsDoneAndStartNew(currentThread); } } } ); } else { // process any incoming joins (they will fail because we are not the master) nodeJoinController.stopElectionContext(masterNode + &quot; elected&quot;); // send join request final boolean success = joinElectedMaster(masterNode); synchronized (stateMutex) { if (success) { DiscoveryNode currentMasterNode = this.clusterState().getNodes().getMasterNode(); if (currentMasterNode == null) { // Post 1.3.0, the master should publish a new cluster state before acking our join request. we now should have // a valid master. logger.debug(&quot;no master node is set, despite of join request completing. retrying pings.&quot;); joinThreadControl.markThreadAsDoneAndStartNew(currentThread); } else if (currentMasterNode.equals(masterNode) == false) { // update cluster state joinThreadControl.stopRunningThreadAndRejoin(&quot;master_switched_while_finalizing_join&quot;); } joinThreadControl.markThreadAsDone(currentThread); } else { // failed to join. Try again... joinThreadControl.markThreadAsDoneAndStartNew(currentThread); } } } 按照上述流程，我们描述一个简单的场景来帮助大家理解： 假如集群中有3个master-eligible node，分别为Node_A、 Node_B、 Node_C, 选举优先级也分别为Node_A、Node_B、Node_C。三个node都认为当前没有master，于是都各自发起选举，选举结果都为Node_A(因为选举时按照优先级排序，如上文所述)。于是Node_A开始等join(选票)，Node_B、Node_C都向Node_A发送join，当Node_A接收到一次join时，加上它自己的一票，就获得了两票了(超过半数)，于是Node_A成为Master。此时cluster_state(集群状态)中包含两个节点，当Node_A再收到另一个节点的join时，cluster_state包含全部三个节点。 选举怎么保证不脑裂？基本原则还是多数派的策略，如果必须得到多数派的认可才能成为Master，那么显然不可能有两个Master都得到多数派的认可。 上述流程中，master候选人需要等待多数派节点进行join后才能真正成为master，就是为了保证这个master得到了多数派的认可。但是我这里想说的是，上述流程在绝大部份场景下没问题，听上去也非常合理，但是却是有bug的。 因为上述流程并没有限制在选举过程中，一个Node只能投一票，那么什么场景下会投两票呢？比如NodeB投NodeA一票，但是NodeA迟迟不成为Master，NodeB等不及了发起了下一轮选主，这时候发现集群里多了个Node0，Node0优先级比NodeA还高，那NodeB肯定就改投Node0了。假设Node0和NodeA都处在等选票的环节，那显然这时候NodeB其实发挥了两票的作用，而且投给了不同的人。 那么这种问题应该怎么解决呢，比如raft算法中就引入了选举周期(term)的概念，保证了每个选举周期中每个成员只能投一票，如果需要再投就会进入下一个选举周期，term+1。假如最后出现两个节点都认为自己是master，那么肯定有一个term要大于另一个的term，而且因为两个term都收集到了多数派的选票，所以多数节点的term是较大的那个，保证了term小的master不可能commit任何状态变更(commit需要多数派节点先持久化日志成功，由于有term检测，不可能达到多数派持久化条件)。这就保证了集群的状态变更总是一致的。 而ES目前(6.2版本)并没有解决这个问题，构造类似场景的测试case可以看到会选出两个master，两个node都认为自己是master，向全集群发布状态变更，这个发布也是两阶段的，先保证多数派节点“接受”这次变更，然后再要求全部节点commit这次变更。很不幸，目前两个master可能都完成第一个阶段，进入commit阶段，导致节点间状态出现不一致，而在raft中这是不可能的。那么为什么都能完成第一个阶段呢，因为第一个阶段ES只是将新的cluster_state做简单的检查后放入内存队列，如果当前cluster_state的master为空，不会对新的clusterstate中的master做检查，即在接受了NodeA成为master的cluster_state后(还未commit)，还可以继续接受NodeB成为master的cluster_state。这就使NodeA和NodeB都能达到commit条件，发起commit命令，从而将集群状态引向不一致。当然，这种脑裂很快会自动恢复，因为不一致发生后某个master再次发布cluster_state时就会发现无法达到多数派条件，或者是发现它的follower并不构成多数派而自动降级为candidate等。 这里要表达的是，ES的ZenDiscovery模块与成熟的一致性方案相比，在某些特殊场景下存在缺陷，下一篇文章讲ES的meta变更流程时也会分析其他的ES无法满足一致性的场景。 错误检测MasterFaultDetection与NodesFaultDetection这里的错误检测可以理解为类似心跳的机制，有两类错误检测，一类是Master定期检测集群内其他的Node，另一类是集群内其他的Node定期检测当前集群的Master。检查的方法就是定期执行ping请求。ES文档： There are two fault detection processes running. The first is by the master, to ping all the other nodes in the cluster and verify that they are alive. And on the other end, each node pings to master to verify if its still alive or an election process needs to be initiated.如果Master检测到某个Node连不上了，会执行removeNode的操作，将节点从cluste_state中移除，并发布新的cluster_state。当各个模块apply新的cluster_state时，就会执行一些恢复操作，比如选择新的primaryShard或者replica，执行数据复制等。 如果某个Node发现Master连不上了，会清空pending在内存中还未commit的new cluster_state，然后发起rejoin，重新加入集群(如果达到选举条件则触发新master选举)。 rejoin除了上述两种情况，还有一种情况是Master发现自己已经不满足多数派条件(&gt;=minimumMasterNodes)了，需要主动退出master状态(退出master状态并执行rejoin)以避免脑裂的发生，那么master如何发现自己需要rejoin呢？ 上面提到，当有节点连不上时，会执行removeNode。在执行removeNode时判断剩余的Node是否满足多数派条件，如果不满足，则执行rejoin。 if (electMasterService.hasEnoughMasterNodes(remainingNodesClusterState.nodes()) == false) { final int masterNodes = electMasterService.countMasterNodes(remainingNodesClusterState.nodes()); rejoin.accept(LoggerMessageFormat.format(&quot;not enough master nodes (has [{}], but needed [{}])&quot;, masterNodes, electMasterService.minimumMasterNodes())); return resultBuilder.build(currentState); } else { return resultBuilder.build(allocationService.deassociateDeadNodes(remainingNodesClusterState, true, describeTasks(tasks))); } 在publish新的cluster_state时，分为send阶段和commit阶段，send阶段要求多数派必须成功，然后再进行commit。如果在send阶段没有实现多数派返回成功，那么可能是有了新的master或者是无法连接到多数派个节点等，则master需要执行rejoin。 try { publishClusterState.publish(clusterChangedEvent, electMaster.minimumMasterNodes(), ackListener); } catch (FailedToCommitClusterStateException t) { // cluster service logs a WARN message logger.debug(&quot;failed to publish cluster state version [{}] (not enough nodes acknowledged, min master nodes [{}])&quot;, newState.version(), electMaster.minimumMasterNodes()); synchronized (stateMutex) { pendingStatesQueue.failAllStatesAndClear( new ElasticsearchException(&quot;failed to publish cluster state&quot;)); rejoin(&quot;zen-disco-failed-to-publish&quot;); } throw t; } 在对其他节点进行定期的ping时，发现有其他节点也是master，此时会比较本节点与另一个master节点的cluster_state的version，谁的version大谁成为master，version小的执行rejoin。 if (otherClusterStateVersion &gt; localClusterState.version()) { rejoin(&quot;zen-disco-discovered another master with a new cluster_state [&quot; + otherMaster + &quot;][&quot; + reason + &quot;]&quot;); } else { // TODO: do this outside mutex logger.warn(&quot;discovered [{}] which is also master but with an older cluster_state, telling [{}] to rejoin the cluster ([{}])&quot;, otherMaster, otherMaster, reason); try { // make sure we&apos;re connected to this node (connect to node does nothing if we&apos;re already connected) // since the network connections are asymmetric, it may be that we received a state but have disconnected from the node // in the past (after a master failure, for example) transportService.connectToNode(otherMaster); transportService.sendRequest(otherMaster, DISCOVERY_REJOIN_ACTION_NAME, new RejoinClusterRequest(localClusterState.nodes().getLocalNodeId()), new EmptyTransportResponseHandler(ThreadPool.Names.SAME) { @Override public void handleException(TransportException exp) { logger.warn((Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(&quot;failed to send rejoin request to [{}]&quot;, otherMaster), exp); } }); } catch (Exception e) { logger.warn((Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(&quot;failed to send rejoin request to [{}]&quot;, otherMaster), e); } } 集群扩缩容上面讲了节点发现、Master选举、错误检测等机制，那么现在我们可以来看一下如何对集群进行扩缩容。 扩容DataNode假设一个ES集群存储或者计算资源不够了，我们需要进行扩容，这里我们只针对DataNode，即配置为： conf/elasticsearch.yml: node.master: false node.data: true 然后需要配置集群名、节点名等其他配置，为了让该节点能够加入集群，我们把discovery.zen.ping.unicast.hosts配置为集群中的master-eligible node。 conf/elasticsearch.yml: cluster.name: es-cluster node.name: node_Z discovery.zen.ping.unicast.hosts: [&quot;x.x.x.x&quot;, &quot;x.x.x.y&quot;, &quot;x.x.x.z&quot;] 然后启动节点，节点会自动加入到集群中，集群会自动进行rebalance，或者通过reroute api进行手动操作。 https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html https://www.elastic.co/guide/en/elasticsearch/reference/current/shards-allocation.html 缩容DataNode假设一个ES集群使用的机器数太多了，需要缩容，我们怎么安全的操作来保证数据安全，并且不影响可用性呢？ 首先，我们选择需要缩容的节点，注意本节只针对DataNode的缩容，MasterNode缩容涉及到更复杂的问题，下面再讲。 然后，我们需要把这个Node上的Shards迁移到其他节点上，方法是先设置allocation规则，禁止分配Shard到要缩容的机器上，然后让集群进行rebalance。 PUT _cluster/settings { &quot;transient&quot; : { &quot;cluster.routing.allocation.exclude._ip&quot; : &quot;10.0.0.1&quot; } } 等这个节点上的数据全部迁移完成后，节点可以安全下线。 更详细的操作方式可以参考官方文档： https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-filtering.html 扩容MasterNode假如我们想扩容一个MasterNode(master-eligible node)， 那么有个需要考虑的问题是，上面提到为了避免脑裂，ES是采用多数派的策略，需要配置一个quorum数： conf/elasticsearch.yml: discovery.zen.minimum_master_nodes: 2 假设之前3个master-eligible node，我们可以配置quorum为2，如果扩容到4个master-eligible node，那么quorum就要提高到3。 所以我们应该先把discovery.zen.minimum_master_nodes这个配置改成3，再扩容master，更改这个配置可以通过API的方式： curl -XPUT localhost:9200/_cluster/settings -d &apos;{ &quot;persistent&quot; : { &quot;discovery.zen.minimum_master_nodes&quot; : 3 } } 这个API发送给当前集群的master，然后新的值立即生效，然后master会把这个配置持久化到cluster meta中，之后所有节点都会以这个配置为准。 但是这种方式有个问题在于，配置文件中配置的值和cluster meta中的值很可能出现不一致，不一致很容易导致一些奇怪的问题，比如说集群重启后，在恢复cluster meta前就需要进行master选举，此时只可能拿配置中的值，拿不到cluster meta中的值，但是cluster meta恢复后，又需要以cluster meta中的值为准，这中间肯定存在一些正确性相关的边界case。 总之，动master节点以及相关的配置一定要谨慎，master配置错误很有可能导致脑裂甚至数据写坏、数据丢失等场景。 缩容MasterNode缩容MasterNode与扩容跟扩容是相反的流程，我们需要先把节点缩下来，再把quorum数调下来，不再详细描述 与Zookeeper、raft等实现方式的比较与使用Zookeeper相比本篇讲了ES集群中节点相关的几大功能的实现方式： 节点发现 Master选举 错误检测 集群扩缩容 试想下，如果我们使用Zookeeper来实现这几个功能，会带来哪些变化？ Zookeeper介绍我们首先介绍一下Zookeeper，熟悉的同学可以略过。 Zookeeper分布式服务框架是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。 简单来说，Zookeeper就是用于管理分布式系统中的节点、配置、状态，并完成各个节点间配置和状态的同步等。大量的分布式系统依赖Zookeeper或者是类似的组件。 Zookeeper通过目录树的形式来管理数据，每个节点称为一个znode，每个znode由3部分组成: stat. 此为状态信息, 描述该znode的版本, 权限等信息. data. 与该znode关联的数据. children. 该znode下的子节点. stat中有一项是ephemeralOwner，如果有值，代表是一个临时节点，临时节点会在session结束后删除，可以用来辅助应用进行master选举和错误检测。 Zookeeper提供watch功能，可以用于监听相应的事件，比如某个znode下的子节点的增减，某个znode本身的增减，某个znode的更新等。 怎么使用Zookeeper实现ES的上述功能 节点发现：每个节点的配置文件中配置一下Zookeeper服务器的地址，节点启动后到Zookeeper中某个目录中注册一个临时的znode。当前集群的master监听这个目录的子节点增减的事件，当发现有新节点时，将新节点加入集群。 master选举：当一个master-eligible node启动时，都尝试到固定位置注册一个名为master的临时znode，如果注册成功，即成为master，如果注册失败则监听这个znode的变化。当master出现故障时，由于是临时znode，会自动删除，这时集群中其他的master-eligible node就会尝试再次注册。使用Zookeeper后其实是把选master变成了抢master。 错误检测：由于节点的znode和master的znode都是临时znode，如果节点故障，会与Zookeeper断开session，znode自动删除。集群的master只需要监听znode变更事件即可，如果master故障，其他的候选master则会监听到master znode被删除的事件，尝试成为新的master。 集群扩缩容：扩缩容将不再需要考虑minimum_master_nodes配置的问题，会变得更容易。 使用Zookeeper的优劣点使用Zookeeper的好处是，把一些复杂的分布式一致性问题交给Zookeeper来做，ES本身的逻辑就可以简化很多，正确性也有保证，这也是大部分分布式系统实践过的路子。而ES的这套ZenDiscovery机制经历过很多次bug fix，到目前仍有一些边角的场景存在bug，而且运维也不简单。 那为什么ES不使用Zookeeper呢，大概是官方开发觉得增加Zookeeper依赖后会多依赖一个组件，使集群部署变得更复杂，用户在运维时需要多运维一个Zookeeper。 那么在自主实现这条路上，还有什么别的算法选择吗？当然有的，比如raft。 与使用raft相比raft算法是近几年很火的一个分布式一致性算法，其实现相比paxos简单，在各种分布式系统中也得到了应用。这里不再描述其算法的细节，我们单从master选举算法角度，比较一下raft与ES目前选举算法的异同点： 相同点 多数派原则：必须得到超过半数的选票才能成为master。 选出的leader一定拥有最新已提交数据：在raft中，数据更新的节点不会给数据旧的节点投选票，而当选需要多数派的选票，则当选人一定有最新已提交数据。在es中，version大的节点排序优先级高，同样用于保证这一点。 不同点 正确性论证：raft是一个被论证过正确性的算法，而ES的算法是一个没有经过论证的算法，只能在实践中发现问题，做bug fix，这是我认为最大的不同。 是否有选举周期term：raft引入了选举周期的概念，每轮选举term加1，保证了在同一个term下每个参与人只能投1票。ES在选举时没有term的概念，不能保证每轮每个节点只投一票。 选举的倾向性：raft中只要一个节点拥有最新的已提交的数据，则有机会选举成为master。在ES中，version相同时会按照NodeId排序，总是NodeId小的人优先级高。看法raft从正确性上看肯定是更好的选择，而ES的选举算法经过几次bug fix也越来越像raft。当然，在ES最早开发时还没有raft，而未来ES如果继续沿着这个方向走很可能最终就变成一个raft实现。 Elasticsearch分布式一致性原理剖析(一)-节点篇 Elasticsearch内核解析 - 写入篇 ElasticSearch权威指南]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ基本介绍]]></title>
    <url>%2F2019%2F06%2F15%2FRocketMQ%2F</url>
    <content type="text"><![CDATA[RabbitMQ基本概念生产者和消费者 Producer：生产者，投递消息的一方。 Consumer: 消费者，就是接收消息的一方。 Broker: 消息中间件的服务节点 。 队列Queue: 队列，是RabbitMQ 的内部对象，用于存储消息。 RabbitMQ中消息都只能存储在队列中。 多个消费者可以订阅同一个队列，这时队列中的消息会被平均分摊 CRound-Robin ，即轮询)给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理。 RabbitMQ 不支持队列层面的广播消费。 交换器、路由键与绑定Exchange: 交换器。生产者将消息发送到 Exchange (交换器，通常也可以用大写的 “X” 来表示)，由交换器将消息路由到一个或者多个队列中。如果路由不到，或许会返回给生产者，或许直接丢弃。 RoutingKey: 路由键 。生产者将消息发给交换器 的时候，一般会指定一个RoutingKey ，用来指定这个消息的路由规则，而这个 RoutingKey 需要与交换器类型和绑定键 (BindingKey) 联合使用才能最终生效。 Binding: 绑定。RabbitMQ 中通过绑定将交换器与队列关联起来，在绑定的时候一般会指定一个绑定键 ( BindingKey ) ，这样 RabbitMQ 就知道如何正确地将消息路由到队列了。 在交换器类型和绑定键 (BindingKey) 固定的情况下，生产者可以在发送消息给交换器时，通过指定RoutingKey来决定消息流向哪里。 交换器类型RabbitMQ 常用的交换器类型有 fanout 、 direct 、 topic 、 headers 这四种 。 fanout它会把所有发送到该交换器的消息路由到所有与该交换器绑定的队列中。 directdirect 类型的交换器路由规则也很简单，它会把消息路由到那些 BindingKey 和 RoutingKey完全匹配的队列中。 topictopic 类型的交换器在匹配规则上进行了扩展，它与 direct 类型的交换器相似，也是将消息路由到 BindingKey 和 RoutingKey 相匹配的队列中，但这里的匹配规则有些不同，它约定: RoutingKey 为一个点号”.”分隔的字符串(被点号”.”分隔开的每一段独立的字符串称为一个单词)，如com.rabbitmq.client BindingKey 和 RoutingKey 一样也是点号”.”分隔的字符串; BindingKey 中可以存在两种特殊字符串”“和”#”，用于做模糊匹配，其中”“用于匹配一个单词，”#”用于匹配多规格单词(可以是零个)。 headersheaders 类型的交换器不依赖于路由键的匹配规则来路由消息，而是根据发送的消息内容中的 headers 属性进行匹配。在绑定队列和交换器时制定一组键值对 ， 当发送消息到交换器时，RabbitMQ 会获取到该消息的 headers (也是一个键值对的形式) ，对比其中的键值对是否完全匹配队列和交换器绑定时指定的键值对，如果完全匹配则消息会路由到该队列，否则不会路由到该队列 。 headers 类型的交换器性能会很差，而且也不实用，基本上不会看到它的存在。 RabbitMQ 运转流程生产者发送消息： 生产者连接到 RabbitMQ Broker，建立一个连接(Connection),开启一个信道 (Channel) 生产者声明一个交换器 ，并设置相关属性，比如 交换机类型、是否持久化等。 生产者声明 一个队列井设置相关属性，比如是否排他、是否持久化、是否自动删除等。 生产者通过路由键将交换器和队列绑定起来。 生产者发送消息至 RabbitMQ Broker，其中包含路由键、交换器等信息。 相应的交换器根据接收到的路由键查找相匹配的队列 。 如果找到，则将从生产者发送过来的消息存入相应的队列中。 如果没有找到，则根据生产者配置的属性选择丢弃还是回退给生产者。 关闭信道。 关闭连接。 消费者接收消息的过程: 消费者连接到 RabbitMQ Broker，建立一个连接 (Connection ) ，开启 一个信道 (Channel) 。 消费者向 RabbitMQ Broker 请求消费相应队列中的消息，可能会设置相应的回调函数，以及做一些准备工作(详细内容请参考 3 .4节〉。 等待 RabbitMQ Broker 回应并投递相应队列中的消息， 消费者接收消息。 消费者确认 ( ack) 接收到的消息 。 RabbitMQ 从队列中删除相应己经被确认的消息 。 关闭信道。 关闭连接。 RabbitMQ进阶消息特殊情况mandatory 和 immediate 是 channel . basicPublish 方法中的两个参数，它们都有当消息传递过程中不可达目的地时将消息返回给生产者的功能。 RabbitMQ 提供的备份交换器(Altemate Exchange) 可以将未能被交换器路由的消息(没有绑定队列或者没有匹配的绑定〉存储起来，而不用返回给客户端。 mandatory参数当 mandatory 参数设为 true 时，交换器无法根据自身的类型和路由键找到一个符合条件的队列，那么 RabbitMQ 会调用 Basic.Return 命令将消息返回给生产者 。当 mandatory 参数设置为 false 时，出现上述情形，则消息直接被丢弃 。 那么生产者如何获取到没有被正确路由到合适队列的消息呢?这时候可以通过调用channel.addReturnListener 来添加 ReturnListener 监昕器实现。 immediate参数当 imrnediate 参数设为 true 时，如果交换器在将消息路由到队列时发现队列上并不存在任何消费者，那么这条消息将不会存入队列中。当与路由键匹配的所有队列都没有消费者时 ，该消息会通过 Basic.Return 返回至生产者。 RabbitMQ3.0版本开始去掉了对 imrnediate 参数的支持，对此 RabbitMQ 官方解释是:imrnediate 参数会影响镜像队列的性能，增加了代码复杂性，建议采用TTL和DLX 的方法替代。 备份交换器备份交换器，英文名称为 Altemate Exchange，简称庙，或者更直白地称之为”备胎交换器”。生产者在发送消息的时候如果不设置 mandatory 参数 ， 那么消息在未被路由的情况下将会丢失 :如果设置了 mandatory 参数，那么需要添加 ReturnListener 的编程逻辑，生产者的代码将变得复杂。如果既不想复杂化生产者的编程逻辑，又不想消息丢失，那么可以使用备份交换器，这样可以将未被路由的消息存储在 RabbitMQ 中，再在需要的时候去处理这些消息。 可以通过在声明交换器(调用channel.exchangeDeclare方法)的时候添加alternate-exchange 参数来实现，也可以通过策略的方式实现。如果两者同时使用，则前者的优先级更高，会覆盖掉 Policy 的设置 。 TTL设置消息的 TTL目前有两种方法可以设置消息的 TTL。第一种方法是通过队列属性设置，队列中所有消息都有相同的过期时间。第二种方法是对消息本身进行单独设置，每条消息的 TTL 可以不同。如果两种方法一起使用，则消息的 TTL 以两者之间较小的那个数值为准。消息在队列中的生存时间一旦超过设置 的 TTL 值时，就会变成”死信” (Dead Message) ，消费者将无法再收到该消息。 通过队列属性设置消息 TTL 的方法是在 channel.queueDeclare 方法中加入x-message -ttl 参数实现的，这个参数的单位是毫秒。 Map&lt;String, Object&gt; argss = new HashMap&lt;String , Object&gt;(); argss.put(&quot;x-message-ttl &quot; , 6000); channel.queueDeclare(queueName , durable , exclusive , autoDelete , argss) ; //同时也可以通过 Policy 的方式来设置 TTL.示例如下 : rabbitmqctl set_policy TTL &quot;食&quot; &apos;{&quot;message-ttl&quot;:60000}&apos; --apply-to queues //还可以通过调用 HTTPAPI 接口设置 : $ curl -i -u root:root -H &quot;content-type:application/json&quot;-X PUT -d&apos;{&quot;auto_delete&quot;:false, &quot;durable&quot;:true, &quot;arguments&quot;:{&quot;x-message-ttl&quot;: 60000}}&apos; http://localhost:15672/api/queues/{vhost}/{queuename} 针对每条消息设置 TTL 的方法是在 channel.basicPublish 方法中加入 expiration的属性参数，单位为毫秒。 AMQP.BasicProperties.Builder builder = new AMQP.BasicProperties . Builder(); builder.deliveryMode(2); / / 持久化消息 builder.expiration( &quot; 60000 &quot; );/ / 设置 TTL=60000ms AMQP.BasicProperties properties = builder.build() ; channel.basicPublish(exchangeName, routingKey, mandatory, properties , &quot;ttlTestMessage&quot;.getBytes()); 如果不设置 TTL.则表示此消息不会过期 ;如果将 TTL 设置为 0，则表示除非此时可以直接将消息投递到消费者，否则该消息会被立即丢弃，这个特性可以部分替代 RabbitMQ 3.0 版本之前的 immediate 参数，之所以部分代替，是因为immediate 参数在投递失败时会用Basic.Return 将消息返回。 对于第一种设置队列 TTL 属性的方法，一旦消息过期，就会从队列中抹去，而在第二种方法中，即使消息过期，也不会马上从队列中抹去，因为每条消息是否过期是在即将投递到消费者之前判定的。 为什么这两种方法处理的方式不一样?因为第一种方法里，队列中己过期的消息肯定在队列头部， RabbitMQ 只要定期从队头开始扫描是否有过期的消息即可。而第二种方法里，每条消息的过期时间不同，如果要删除所有过期消息势必要扫描整个队列，所以不如等到此消息即将被消费时再判定是否过期 ， 如果过期再进行删除即可。 设置队列的TTL通过 channel.queueDeclare 方法中的 x-expires 参数可以控制队列被自动删除前处于未使用状态的时间。未使用的意思是队列上没有任何的消费者，队列也没有被重新声明，并且在过期时间段内也未调用过Basic.Get命令。 abb itMQ 会确保在过期时间到达后将队列删除，但是不保障删除的动作有多及时 。在RabbitMQ 重启后 ， 持久化的队列的过期时间会被重新计算。用于表示过期时间的 x-expires 参数以毫秒为单位 ， 井且服从和 x-message-ttl 一样的约束条件，不过不能设置为 0。比如该参数设置为 1 000 ，则表示该队列如果在 1 秒钟之内未使用则会被删除。 死信队列DLX ，全称为 Dead-Letter-Exchange ，可以称之为死信交换器，也有人称之为死信邮箱。当消息在一个队列中变成死信 (dead message) 之后，它能被重新被发送到另一个交换器中，这个交换器就是 DLX，绑定 DLX 的队列就称之为死信队列。 DLX 也是一个正常的交换器，和一般的交换器没有区别，它能在任何的队列上被指定，实际上就是设置某个队列的属性。当这个队列中存在死信时，RabbitMQ 就会自动地将这个消息重新发布到设置的 DLX 上去 ，进而被路由到另一个队列，即死信队列。可以监听这个队列中的消息、以进行相应的处理，这个特性与将消息的 TTL 设置为 0 配合使用可以弥补imrnediate参数功能。 延迟队列延迟队列存储的对象是对应的延迟消息，所谓”延迟消息”是指当消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费 。 在图 4-4 中，不仅展示的是死信队列的用法，也是延迟队列的用法，对于 queue.dlx 这个死信队列来说，同样可以看作延迟队列。假设一个应用中需要将每条消息都设置为 10 秒的延迟，生产者通过exchange.normal 这个交换器将发送的消息存储在 queue.normal 这个队列中。消费者订阅的并非是 queue.normal 这个队列，而是 queue.dlx 这个队列 。当消息从 queue.normal 这个队列中过期之后被存入 queue.dlx 这个队列中，消费者就恰巧消费到了延迟 10 秒的这条消息 。 优先队列优先级队列，顾名思义，具有高优先级的队列具有高的优先权，优先级高的消息具备优先被消费的特权。可以通过设置队列的 x-max-priority 参数来实现。 RPC实现 RPC 的主要功用是让构建分布式计算更容易，在提供强大的远程调用能力时不损失本地调用的语义简洁性。 一般在 RabbitMQ 中进行 RPC 是很简单。客户端发送请求消息，服务端回复响应的消息 。为了接收响应的消息，我们需要在请求消息中发送一个回调队列。 这里就用到两个属性。 replyTo: 通常用来设置一个回调队列。 correlationId : 用来关联请求(request) 和其调用RPC之后的回复(response ) 。 RPC 的处理流程如下 : 当客户端启动时，创建一个匿名的回调队列(名称由 RabbitMQ 自动创建，图 4-7 中的回调队列为 amq.gen-LhQzlgv3GhDOv8PIDabOXA 。 客户端为 RPC 请求设置 2 个属性 : reply T o 用来告知 RPC 服务端回复请求时的目的队列，即回调队列; correlationld 用来标记一个请求。 请求被发送到 rpc_queue 队列中。 RPC 服务端监听 rpc_queue 队列中的请求，当请求到来时 ， 服务端会处理并且把带有结果的消息发送给客户端。 接收的队列就是 replyTo 设定 的 回调队列。 客户端监昕回调队列，当有消息时 ， 检查 correlationld 属性，如果与请求匹配，那就是结果了。 持久化RabbitMQ的持久化分为三个部分:交换器的持久化、队列的持久化和消息的持久化 。 交换器的持久化是通过在声明队列是将 durable 参数置为 true 实现的。 队列的持久化是通过在声明队列时将 durable 参数置为 true 实现的。 队列的持久化能保证其本身的元数据不会因异常情况而丢失，但是并不能保证内部所存储的消息不会丢失。要确保消息不会丢失 ， 需要将其设置为持久化。通过将消息的投递模式(BasicProperties 中的deliveryMode 属性)设置为 2 即可实现消息的持久化。 生产者确认消息的生产者将消息发送出去之后，消息到底有没有正确地到达服务器呢?如果不进行特殊配置，默认情况下发送消息的操作是不会返回任何信息给生产者的，也就是默认情况下生产者是不知道消息有没有正确地到达服务器。如果在消息到达服务器之前己经丢失，持久化操作也解决不了这个问题，因为消息根本没有到达服务器 ，何谈持久化? RabbitMQ 针对这个问题，提供了两种解决方式: 通过事务机制实现 通过发送方确认publisher confirm 机制实现。 事务机制Rabb itMQ 客户端中与事务机制相关的方法有 三 个: channel.txSelect 、channel .txCommit 和 channel.txRollbacko。channel.txSelect 用于将当前的信道设置成事务模式。channel.txCommit 用于提交事务。channel.txRollback 用于事务回滚。 在通过 channel.txSelect 方法开启事务之后，我们便可以发布消息给 RabbitMQ 了，如果事务提交成功，则消息一定到达了 RabbitMQ 中，如果在事务提交执行之前由于 RabbitMQ异常崩溃或者其他原因抛出异常，这个时候我们便可以将其捕获，进而通过执行channel.txRollback 方法来实现事务回夜。注意这里的 RabbitMQ 中的事务机制与大多数数据库中的事务概念井不相同，需要注意区分。 务确实能够解决消息发送方和 RabbitMQ 之间消息确认的问题，只有消息成功被RabbitMQ 接收，事务才能提交成功，否则便可在捕获异常之后进行事务回滚 ，与此同时可以进行消息重发。但是使用事务机制会”吸干” RabbitMQ 的性能。 发送方确认机制生产者将信道设置成 confirmn （确认)模式，一旦信道进入 confmn 模式，所有在该信道上面发布的消息都会被指派一个唯一的 ID（从1开始)，一旦消息被投递到所有匹配的队列之后，RabbitMQ 就会发送一个确认 （Basic.Ack) 给生产者(包含消息的唯一 ID) ，这就使得生产者知晓消息已经正确到达了目的地了。如果消息和队列是可持久化的，那么确认消息会在消息写入磁盘之后发出。 RabbitMQ 回传给生产者的确认消息中的 deliveryTag 包含了确认消息的序号，此外 RabbitMQ 也可以设置 channel.basicAck 方法中的multiple 参数，表示到这个序号之前的所有消息都己经得到了处理。 发送方确认机制最大的好处在于它是异步的，一旦发布一条消息，生产者应用程序就可以在等信道返回确认的同时继续发送下一条消息，当消息最终得到确认之后，生产者应用程序便可以通过回调方法来处理该确认消息，如果 RabbitMQ 因为自身内部错误导致消息丢失，就会发送一条 nack（Basic.Nack) 命令，生产者应用程序同样可以在回调方法中处理该 nack 命令。 RabbitMQ实战指南]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7部署redis伪集群]]></title>
    <url>%2F2019%2F06%2F03%2Fcentos7%E9%83%A8%E7%BD%B2redis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[环境系统： centos7 Redis： redis-3.2.4 rbenv: 2.6.3 安装目录： /root/temp/ 部署流程 下载redis并上传到服务器并解压到/root/temp/redis-3.2.4 编译redis，在/root/temp/redis-3.2.4 目录下输入make 安装ruby 安装rbenv #install build dependencies sudo yum install -y git-core zlib zlib-devel gcc-c ++ patch readline readline-devel libyaml-devel libffi-devel openssl-devel make bzip2 autoconf automake libtool bison curl sqlite-devel #clim并安装rbenv环境 cd~ git clone git：//github.com/sstephenson/rbenv.git .rbenv git clone git：//github.com/sstephenson/ruby-build.git~ / .rbenv / plugins / ruby​​-build echo&apos;export PATH =“$ HOME / .rbenv / bin：$ HOME / .rbenv / plugins / ruby​​-build / bin：$ PATH”&apos;&gt;&gt;〜/ .bash_profile echo&apos;eval“$（rbenv init - ）”&apos;&gt;&gt;〜/ .bash_profile #re-init bash source~ / .bash_profile #install最新的ruby rbenv install -v 2.6.3 ＃设置shell将使用的默认ruby版本 rbenv global 2.6.3 ＃禁用生成文档，因为这需要花费很多时间 echo“gem： - no-document”&gt;〜/ .gemrc #install bundler gem install bundler 每次安装gem时都必须执行＃，以便运行ruby可执行文件 rbenv rehash 在使用上面的安装ruby如果出现无法下载文件，此时可以通过下载好ruby，我选择的是最新ruby-2.6.3，上传到服务器/root/temp/目录下，进入/root/.rbenv/plugins/ruby-build/share/ruby-build目录下，找到对应的文件也就是2.6.3，打开这个文件，修改 install_package &quot;openssl-1.0.2j&quot; &quot;https://www.openssl.org/source/openssl-1.0.2j.tar.gz#e7aff292be21c259c6af26469c7a9b3ba26e9abaaffd325e3dccc9785256c431&quot; mac_openssl --if has_broken_mac_openssl install_package &quot;ruby-2.6.3&quot; &quot;file:///root/temp/ruby-2.6.3.tar.gz&quot; 如果使用rvm来安装ruby，在本机上会出现CA证书过期无法下载文件。 配置redis集群 在/root/temp/redis-3.2.4目录下新建目录redis-cluster 在目录redis-cluster下创建7001 7002 7003 7004 7005 7006 目录 将/root/temp/redis-3.2.4目录下的redis.conf配置文件复制到7001 … 7006 目录下 分配修改7001-7006目录下的redis.conf 文件修改为对应的端口号 port 7001 #要根据所在的子目录下配置 daemonize yes pidfile /var/run/redis_7001.pid #要根据所在的子目录下配置 logfile &quot;/var/log/redis-7001.log&quot; #要根据所在的子目录下配置 appendonly yes cluster-enabled yes cluster-config-file nodes-7001.conf #要根据所在的子目录下配置 cluster-node-timeout 15000 启动redis - 在对应目录下输入命令，逐个启动 cd 7001 /root/temp/redis-3.2.4/src/redis-server redis.conf cd .. cd 7002 /root/temp/redis-3.2.4/src/redis-server redis.conf cd .. cd 7003 cd .. /root/temp/redis-3.2.4/src/redis-server redis.conf cd 7004 cd .. /root/temp/redis-3.2.4/src/redis-server redis.conf cd .. cd 7005 /root/temp/redis-3.2.4/src/redis-server redis.conf cd 7006 /root/temp/redis-3.2.4/src/redis-server redis.conf 启动redis集群 — 在/root/temp/redis-3.2.4目录下输入 src/redis-trib create --replicas 1 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 rbenv安装ruby2.3.0在线安装不上。老子出绝招了(更新) https://gist.github.com/soardex/e95cdc230d1ac5b824b3]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[秒杀系统的优化]]></title>
    <url>%2F2019%2F06%2F01%2F%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[秒杀系统简介秒杀场景一般会在电商网站举行一些活动或者节假日在12306网站上抢票时遇到。对于电商网站中一些稀缺或者特价商品，电商网站一般会在约定时间点对其进行限量销售，因为这些商品的特殊性，会吸引大量用户前来抢购，并且会在约定的时间点同时在秒杀页面进行抢购。 秒杀系统主要面临三大问题： 一、瞬时的高并发访问。抢购和普通的电商销售有所不同，普通的电商销售，流量是比较平均的，虽然有波峰波谷，但不会特别突出。而抢购是在特定时间点进行的推销活动，抢购开始前，用户不断刷新页面，以获得购买按钮；抢购开始的一瞬间，集中并发购买。 二、数据正确性。抢购毕竟是一种购买行为，需要购买、扣减库存、支付等复杂的流程，在此过程中，要保证数据的正确性，防止超卖（卖出量超过库存）的发生。 三、防作弊。 秒杀系统的设计理念秒杀系统的优化或者设计理念为以下几点： 限流： 鉴于只有少部分用户能够秒杀成功，所以要限制大部分流量，只允许少部分流量进入服务后端。 削峰：对于秒杀系统瞬时会有大量用户涌入，所以在抢购一开始会有很高的瞬间峰值。高峰值流量是压垮系统很重要的原因，所以如何把瞬间的高流量变成一段时间平稳的流量也是设计秒杀系统很重要的思路。实现削峰的常用的方法有利用缓存和消息中间件等技术。 异步处理：秒杀系统是一个高并发系统，采用异步处理模式可以极大地提高系统并发量，其实异步处理就是削峰的一种实现方式。 内存缓存：秒杀系统最大的瓶颈一般都是数据库读写，由于数据库读写属于磁盘IO，性能很低，如果能够把部分数据或业务逻辑转移到内存缓存，效率会有极大地提升。 可拓展：当然如果我们想支持更多用户，更大的并发，最好就将系统设计成弹性可拓展的，如果流量来了，拓展机器就好了。像淘宝、京东等双十一活动时会增加大量机器应对交易高峰。 前端方案页面静态化： 将页面上的所有静态元素全部静态化，将其交给ngix管理，以此同时采用cdn来N来抗峰值；对于动态部分采用Ajex请求动态加载数据 静态资源优化：主要是讲多个css/js请求合并为一个等。 秒杀接口隐藏：在秒杀开始前一段时间才暴露出秒杀的接口（路径），同时在后端可以通过与用户id绑定生成每个用户的秒杀路径，保存在redis中，在固定时间中只允许固定的次数请求，过多的请求被拦截。这样防止使用脚本等进行大批量的请求。 验证码：主要是为了流量削峰与筛除简单脚本。通过用户输入验证码的时间，将在某一时间的突发流量均摊到随后的一段时间，还能筛除掉一部分简单的无法识别验证码的脚本。 后端方案用户限流：通过对用户id绑定生成的秒杀路径，限定访问次数 redis缓存： 将数据库内容比如秒杀商品内容详情缓存到redis，不访问数据库 消息队列： 通过将请求缓存到消息队列，异步执行来削减同一时间到来的请求。 CDN：内容分发网络。其基本思路是尽可能避开互联网上有可能影响数据传输速度和稳定性的瓶颈和环节，使内容传输的更快、更稳定。简单的来说，就是把原服务器上数据复制到其他服务器上，用户访问时，那台服务器近访问到的就是那台服务器上的数据。CDN的劣势是内容的变更生效慢，所以仅适用于“几乎不变”的资源，例如引用的js包，图片等。 秒杀系统的具体实现秒杀开始前： 只能看到看到秒杀商品详情，无法进入秒杀接口，直到秒杀开始前一段时间。 对进入秒杀商品详情页的用户将其uid与秒杀商品gid结合，生成该用户对该秒杀商品的秒杀接口路径，将其保存到redis中。 在秒杀商品详情页页面采用js脚本来实现倒计时与限制秒杀按钮的电机。 秒杀开始时： 服务端使用redis提前缓存秒杀商品详情，主要参数为库存数量。 使用redis来实现秒杀商品的预减，不直接访问数据库。当redis中的商品库存少于0，拒绝请求。这样能保证只有少量的请求能够接近数据库。 将抢到预减的请求放入到消息队列中，将同步转为异步执行，实现流量的再次削峰。 服务层从消息队列中拿出请求，进行数据库操作，实现真正的商品抢购事务，生成订单；若事务失败，回滚事务，告知用户抢购失败。若事务成功，请求用户确认订单详情，并支付。 注意事项： 防止买超： 需要对数据库字段进行设计，添加索引；在服务端需要对库存字段进行限制，保证不为负。 消息队列： 也可以使用redis来充当消息队列。 用户限流： 限制用户一段时间内能点击的次数 IP限流： 方法太过于粗暴，容易误封无辜用户，不推荐。]]></content>
      <categories>
        <category>java</category>
        <category>高并发</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>高并发</tag>
        <tag>秒杀系统</tag>
        <tag>reids</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis源码分析]]></title>
    <url>%2F2019%2F05%2F28%2Fredis%2F</url>
    <content type="text"><![CDATA[Redis 数据结构简单动态字符串SDS Redis没有直接使用传统字符串表示，而是构建一种名为简单动态字符串的抽象类型，并将SDS用作Redis默认字符串表示。 SDS定义如下： struct sdshdr{ //记录buf数组中已使用字节的数量 // 等于SDS所保存字符串的长度 int len； //记录buf数组中未使用字节数量； int free； //字节数组，用于保存字符串； char buf[]； } SDS与C字符串区别： 常数复杂度获取字符串长度 杜绝缓冲区溢出 减少修改字符串时带来的内存重分配次数 二进制安全 兼容部分C字符串函数 链表 链表节点定义： typedef struct listNode { // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value; } listNode; 链表节点定义： typedef struct list { // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 链表所包含的节点数量 unsigned long len; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key); } list; Redis 的链表实现的特性可以总结如下： 双端： 链表节点带有 prev 和 next 指针， 获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。 无环： 表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL ， 对链表的访问以 NULL 为终点。 带表头指针和表尾指针： 通过 list 结构的 head 指针和 tail 指针， 程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。 带链表长度计数器： 程序使用 list 结构的 len 属性来对 list 持有的链表节点进行计数， 程序获取链表中节点数量的复杂度为 O(1)。 多态： 链表节点使用 void* 指针来保存节点值， 并且可以通过 list 结构的 dup 、 free 、 match 三个属性为节点值设置类型特定函数， 所以链表可以用于保存各种不同类型的值。 字典 Redis 的字典使用哈希表作为底层实现， 一个哈希表里面可以有多个哈希表节点， 而每个哈希表节点就保存了字典中的一个键值对。 哈希表节点 哈希表节点使用 dictEntry 结构表示， 每个 dictEntry 结构都保存着一个键值对： typedef struct dictEntry { // 键 void *key; // 值 union { void *val; uint64_t u64; int64_t s64; } v; // 指向下个哈希表节点，形成链表 struct dictEntry *next; } dictEntry; key 属性保存着键值对中的键， 而 v 属性则保存着键值对中的值， 其中键值对的值可以是一个指针， 或者是一个 uint64_t 整数， 又或者是一个 int64_t 整数。 next 属性是指向另一个哈希表节点的指针， 这个指针可以将多个哈希值相同的键值对连接在一次， 以此来解决键冲突（collision）的问题。 哈希表 typedef struct dictht { // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used; } dictht; table 属性是一个数组， 数组中的每个元素都是一个指向 dict.h/dictEntry 结构的指针， 每个 dictEntry 结构保存着一个键值对。 size 属性记录了哈希表的大小， 也即是 table 数组的大小， 而 used 属性则记录了哈希表目前已有节点（键值对）的数量。 sizemask 属性的值总是等于 size - 1 ， 这个属性和哈希值一起决定一个键应该被放到 table 数组的哪个索引上面。 一个大小为 4 的空哈希表 （没有包含任何键值对） 字典 typedef struct dict { // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */ } dict; 使用链地址头插法发来解决键冲突 Redis rehash 随着操作的不断执行， 哈希表保存的键值对会逐渐地增多或者减少， 为了让哈希表的负载因子（load factor）维持在一个合理的范围之内， 当哈希表保存的键值对数量太多或者太少时， 程序需要对哈希表的大小进行相应的扩展或者收缩。 扩展和收缩哈希表的工作可以通过执行 rehash （重新散列）操作来完成， Redis 对字典的哈希表执行 rehash 的步骤如下： 为字典的 ht[1] 哈希表分配空间， 这个哈希表的空间大小取决于要执行的操作， 以及 ht[0] 当前包含的键值对数量 （也即是ht[0].used 属性的值）： 如果执行的是扩展操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 2^n （2 的 n 次方幂）； 如果执行的是收缩操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used 的 2^n 。 将保存在 ht[0] 中的所有键值对 rehash 到 ht[1] 上面： rehash 指的是重新计算键的哈希值和索引值， 然后将键值对放置到 ht[1] 哈希表的指定位置上。 当 ht[0] 包含的所有键值对都迁移到了 ht[1] 之后 （ht[0] 变为空表）， 释放 ht[0] ， 将 ht[1] 设置为 ht[0] ， 并在 ht[1] 新创建一个空白哈希表， 为下一次 rehash 做准备。 Redis 渐进式reshash 因此， 为了避免 rehash 对服务器性能造成影响， 服务器不是一次性将 ht[0] 里面的所有键值对全部 rehash 到 ht[1] ， 而是分多次、渐进式地将 ht[0] 里面的键值对慢慢地 rehash 到 ht[1] 。 以下是哈希表渐进式 rehash 的详细步骤： 为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。 在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一。 随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。 渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。 哈希表的扩展与收缩 当以下条件中的任意一个被满足时， 程序会自动开始对哈希表执行扩展操作： 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 1 ； 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 5 ； # 负载因子 = 哈希表已保存节点数量 / 哈希表大小 load_factor = ht[0].used / ht[0].size 跳跃表 Redis 的跳跃表由 redis.h/zskiplistNode 和 redis.h/zskiplist 两个结构定义， 其中 zskiplistNode 结构用于表示跳跃表节点， 而 zskiplist结构则用于保存跳跃表节点的相关信息， 比如节点的数量， 以及指向表头节点和表尾节点的指针， 等等。 展示了一个跳跃表示例， 位于图片最左边的是 zskiplist 结构， 该结构包含以下属性： header ：指向跳跃表的表头节点。 tail ：指向跳跃表的表尾节点。 level ：记录目前跳跃表内，层数最大的那个节点的层数（表头节点的层数不计算在内）。 length ：记录跳跃表的长度，也即是，跳跃表目前包含节点的数量（表头节点不计算在内）。 位于 zskiplist 结构右方的是四个 zskiplistNode 结构， 该结构包含以下属性： 层（level）：节点中用 L1 、 L2 、 L3 等字样标记节点的各个层， L1 代表第一层， L2 代表第二层，以此类推。每个层都带有两个属性：前进指针和跨度。前进指针用于访问位于表尾方向的其他节点，而跨度则记录了前进指针所指向节点和当前节点的距离。在上面的图片中，连线上带有数字的箭头就代表前进指针，而那个数字就是跨度。当程序从表头向表尾进行遍历时，访问会沿着层的前进指针进行。 后退（backward）指针：节点中用 BW 字样标记节点的后退指针，它指向位于当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。 分值（score）：各个节点中的 1.0 、 2.0 和 3.0 是节点所保存的分值。在跳跃表中，节点按各自所保存的分值从小到大排列。 成员对象（obj）：各个节点中的 o1 、 o2 和 o3 是节点所保存的成员对象。 跳跃表节点 跳跃表节点的实现由 redis.h/zskiplistNode 结构定义： typedef struct zskiplistNode { // 后退指针 struct zskiplistNode *backward; // 分值 double score; // 成员对象 robj *obj; // 层 struct zskiplistLevel { // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; } level[]; } zskiplistNode; 与红黑树等平衡树相比，跳跃表具有以下优点： 插入速度非常快速，因为不需要进行旋转等操作来维护平衡性； 更容易实现； 支持无锁操作。 整数集合 整数集合（intset）是 Redis 用于保存整数值的集合抽象数据结构， 它可以保存类型为 int16_t 、 int32_t 或者 int64_t 的整数值， 并且保证集合中不会出现重复元素。 每个 intset.h/intset 结构表示一个整数集合： typedef struct intset { // 编码方式 uint32_t encoding; // 集合包含的元素数量 uint32_t length; // 保存元素的数组 int8_t contents[]; } intset; contents 数组是整数集合的底层实现： 整数集合的每个元素都是 contents 数组的一个数组项（item）， 各个项在数组中按值的大小从小到大有序地排列， 并且数组中不包含任何重复项。 length 属性记录了整数集合包含的元素数量， 也即是 contents 数组的长度。 虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组， 但实际上 contents 数组并不保存任何 int8_t 类型的值 —— contents 数组的真正类型取决于 encoding 属性的值。 升级 每当我们要将一个新元素添加到整数集合里面， 并且新元素的类型比整数集合现有所有元素的类型都要长时， 整数集合需要先进行升级（upgrade）， 然后才能将新元素添加到整数集合里面。 升级整数集合并添加新元素共分为三步进行： 根据新元素的类型， 扩展整数集合底层数组的空间大小， 并为新元素分配空间。 将底层数组现有的所有元素都转换成与新元素相同的类型， 并将类型转换后的元素放置到正确的位上， 而且在放置元素的过程中， 需要继续维持底层数组的有序性质不变。 将新元素添加到底层数组里面。 升级的好处： 提升灵活性： 因为 C 语言是静态类型语言， 为了避免类型错误， 我们通常不会将两种不同类型的值放在同一个数据结构里面。 因为整数集合可以通过自动升级底层数组来适应新元素， 所以我们可以随意地将 int16_t 、 int32_t 或者 int64_t 类型的整数添加到集合中， 而不必担心出现类型错误， 这种做法非常灵活。 节约内存 整数集合现在的做法既可以让集合能同时保存三种不同类型的值， 又可以确保升级操作只会在有需要的时候进行， 这可以尽量节省内存。 降级 整数集合不支持降级操作， 一旦对数组进行了升级， 编码就会一直保持升级后的状态。 压缩列表压缩列表是 Redis 为了节约内存而开发的， 由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构。 一个压缩列表可以包含任意多个节点（entry）， 每个节点可以保存一个字节数组或者一个整数值。 图 7-1 展示了压缩列表的各个组成部分， 表 7-1 则记录了各个组成部分的类型、长度、以及用途。 压缩列表节点的构成 每个压缩列表节点都由 previous_entry_length 、 encoding 、 content 三个部分组成。 previous_entry_length 节点的 previous_entry_length 属性以字节为单位， 记录了压缩列表中前一个节点的长度。previous_entry_length 属性的长度可以是 1 字节或者 5 字节： 如果前一节点的长度小于 254 字节， 那么 previous_entry_length 属性的长度为 1 字节： 前一节点的长度就保存在这一个字节里面。 如果前一节点的长度大于等于 254 字节， 那么 previous_entry_length 属性的长度为 5 字节： 其中属性的第一字节会被设置为 0xFE（十进制值 254）， 而之后的四个字节则用于保存前一节点的长度。 encoding 节点的 encoding 属性记录了节点的 content 属性所保存数据的类型以及长度： 一字节、两字节或者五字节长， 值的最高位为 00 、 01 或者 10 的是字节数组编码： 这种编码表示节点的 content 属性保存着字节数组， 数组的长度由编码除去最高两位之后的其他位记录； 一字节长， 值的最高位以 11 开头的是整数编码： 这种编码表示节点的 content 属性保存着整数值， 整数值的类型和长度由编码除去最高两位之后的其他位记录； content 节点的 content 属性负责保存节点的值， 节点值可以是一个字节数组或者整数， 值的类型和长度由节点的 encoding 属性决定。 其中，字节数组可以是以下三种长度的其中一种： 长度小于等于 63 （2^{6}-1）字节的字节数组； 长度小于等于 16383 （2^{14}-1） 字节的字节数组； 长度小于等于 4294967295 （2^{32}-1）字节的字节数组； 而整数值则可以是以下六种长度的其中一种： 4 位长，介于 0 至 12 之间的无符号整数； 1 字节长的有符号整数； 3 字节长的有符号整数； int16_t 类型整数； int32_t 类型整数； int64_t 类型整数。 连锁更新 在一个压缩列表中， 有多个连续的、长度介于 250 字节到 253 字节之间的节点 e1 至 eN。 因为 e1 至 eN 的所有节点的长度都小于 254 字节， 所以记录这些节点的长度只需要 1 字节长的 previous_entry_length 属性， 换句话说，e1 至 eN 的所有节点的 previous_entry_length 属性都是 1 字节长的。 如果我们将一个长度大于等于 254 字节的新节点 new 设置为压缩列表的表头节点， 那么 new 将成为 e1 的前置节点。 因为 e1 的 previous_entry_length 属性仅长 1 字节， 它没办法保存新节点 new 的长度， 所以程序将对压缩列表执行空间重分配操作， 并将e1 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 现在， 麻烦的事情来了 —— e1 原本的长度介于 250 字节至 253 字节之间， 在为 previous_entry_length 属性新增四个字节的空间之后， e1的长度就变成了介于 254 字节至 257 字节之间， 而这种长度使用 1 字节长的 previous_entry_length 属性是没办法保存的。 因此， 为了让 e2 的 previous_entry_length 属性可以记录下 e1 的长度， 程序需要再次对压缩列表执行空间重分配操作， 并将 e2 节点的previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 正如扩展 e1 引发了对 e2 的扩展一样， 扩展 e2 也会引发对 e3 的扩展， 而扩展 e3 又会引发对 e4 的扩展……为了让每个节点的previous_entry_length 属性都符合压缩列表对节点的要求， 程序需要不断地对压缩列表执行空间重分配操作， 直到 eN 为止。 Redis 将这种在特殊情况下产生的连续多次空间扩展操作称之为“连锁更新”（cascade update）。 对象 Redis 使用对象来表示数据库中的键和值， 每次当我们在 Redis 的数据库中新创建一个键值对时， 我们至少会创建两个对象， 一个对象用作键值对的键（键对象）， 另一个对象用作键值对的值（值对象）。 typedef struct redisObject { // 类型 unsigned type:4; // 编码 unsigned encoding:4; // 指向底层实现数据结构的指针 void *ptr; // ... } robj; 类型 对象的 type 属性记录了对象的类型 类型常量 对象的名称 REDIS_STRING 字符串对象 REDIS_LIST 列表对象 REDIS_HASH 哈希对象 REDIS_SET 集合对象 REDIS_ZSET 有序集合对象 编码 对象的 ptr 指针指向对象的底层实现数据结构， 而这些数据结构由对象的 encoding 属性决定。通过 encoding 属性来设定对象所使用的编码， 而不是为特定类型的对象关联一种固定的编码， 极大地提升了 Redis 的灵活性和效率， 因为 Redis 可以根据不同的使用场景来为一个对象设置不同的编码， 从而优化对象在某一场景下的效率。 编码常量 编码所对应的底层数据结构 REDIS_ENCODING_INT long 类型的整数 REDIS_ENCODING_EMBSTR embstr 编码的简单动态字符串 REDIS_ENCODING_RAW 简单动态字符串 REDIS_ENCODING_HT 字典 REDIS_ENCODING_LINKEDLIST 双端链表 REDIS_ENCODING_ZIPLIST 压缩列表 REDIS_ENCODING_INTSET 整数集合 REDIS_ENCODING_SKIPLIST 跳跃表和字典 对象所使用的底层数据结构 编码常量 OBJECT ENCODING 命令输出 整数 REDIS_ENCODING_INT “int” embstr 编码的简单动态字符串（SDS） REDIS_ENCODING_EMBSTR “embstr” 简单动态字符串 REDIS_ENCODING_RAW “raw” 字典 REDIS_ENCODING_HT “hashtable” 双端链表 REDIS_ENCODING_LINKEDLIST “linkedlist” 压缩列表 REDIS_ENCODING_ZIPLIST “ziplist” 整数集合 REDIS_ENCODING_INTSET “intset” 跳跃表和字典 REDIS_ENCODING_SKIPLIST “skiplist” 字符串对象 字符串对象的编码可以是 int 、 raw 或者 embstr 。 列表对象 列表对象的编码可以是 ziplist 或者 linkedlist 。 当列表对象可以同时满足以下两个条件时， 列表对象使用 ziplist 编码： 列表对象保存的所有字符串元素的长度都小于 64 字节； 列表对象保存的元素数量小于 512 个； 不能满足这两个条件的列表对象需要使用 linkedlist 编码。 注意以上两个条件的上限值是可以修改的， 具体请看配置文件中关于 list-max-ziplist-value 选项和 list-max-ziplist-entries 选项的说明。 哈希对象 哈希对象的编码可以是 ziplist 或者 hashtable 。 ziplist 编码的哈希对象使用压缩列表作为底层实现， 每当有新的键值对要加入到哈希对象时， 程序会先将保存了键的压缩列表节点推入到压缩列表表尾， 然后再将保存了值的压缩列表节点推入到压缩列表表尾， 因此： 保存了同一键值对的两个节点总是紧挨在一起， 保存键的节点在前， 保存值的节点在后； 先添加到哈希对象中的键值对会被放在压缩列表的表头方向， 而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向。 当哈希对象可以同时满足以下两个条件时， 哈希对象使用 ziplist 编码： 哈希对象保存的所有键值对的键和值的字符串长度都小于 64 字节； 哈希对象保存的键值对数量小于 512 个； 不能满足这两个条件的哈希对象需要使用 hashtable 编码。 注意这两个条件的上限值是可以修改的， 具体请看配置文件中关于 hash-max-ziplist-value 选项和 hash-max-ziplist-entries 选项的说明。 集合对象 集合对象的编码可以是 intset 或者 hashtable 。 hashtable 编码的集合对象使用字典作为底层实现， 字典的每个键都是一个字符串对象， 每个字符串对象包含了一个集合元素， 而字典的值则全部被设置为 NULL 。 当集合对象可以同时满足以下两个条件时， 对象使用 intset 编码： 集合对象保存的所有元素都是整数值； 集合对象保存的元素数量不超过 512 个； 不能满足这两个条件的集合对象需要使用 hashtable 编码。 有序集合对象 有序集合的编码可以是 ziplist 或者 skiplist 。 ziplist 编码的有序集合对象使用压缩列表作为底层实现， 每个集合元素使用两个紧挨在一起的压缩列表节点来保存， 第一个节点保存元素的成员（member）， 而第二个元素则保存元素的分值（score）。压缩列表内的集合元素按分值从小到大进行排序， 分值较小的元素被放置在靠近表头的方向， 而分值较大的元素则被放置在靠近表尾的方向。 zset 结构中的 zsl 跳跃表按分值从小到大保存了所有集合元素， 每个跳跃表节点都保存了一个集合元素： 跳跃表节点的 object 属性保存了元素的成员， 而跳跃表节点的 score 属性则保存了元素的分值。 Redis 选择了同时使用字典和跳跃表两种数据结构来实现有序集合。这两种数据结构都会通过指针来共享相同元素的成员和分值， 所以同时使用跳跃表和字典来保存集合元素不会产生任何重复成员或者分值， 也不会因此而浪费额外的内存。 当有序集合对象可以同时满足以下两个条件时， 对象使用 ziplist 编码： 有序集合保存的元素数量小于 128 个； 有序集合保存的所有元素成员的长度都小于 64 字节； 不能满足以上两个条件的有序集合对象将使用 skiplist 编码。 Redis数据库实现数据库 Redis数据库服务器将所有数据库保存在服务器状态redis.h/redisServer结构的db数据汇总 struct redisServer{ //... // 保存服务器中所有数据库 数组 redisDb *db; // 服务器数据库数量，默认为16 int dbnum; }; Redis 是一个键值对（key-value pair）数据库服务器， 服务器中的每个数据库都由一个 redis.h/redisDb 结构表示， 其中， redisDb 结构的dict 字典保存了数据库中的所有键值对， 我们将这个字典称为键空间（key space）： typedef struct redisDb { // ... // 数据库键空间，保存着数据库中的所有键值对 dict *dict; // ... } redisDb; 键的生存时间 EXPIRE key ttl PEXPIRE key ttl EXPIREAT key timestamp PEXPIREAT key timestamp 过期键删除策略 定时删除 惰性删除 定期删除 redis服务器实际使用惰性删除和定期删除相结合来删除过期键。 惰性删除：对输入键进行检查，如果过期就删除键。 定期删除：redis服务器周期性执行activeExpireCycle函数，随机检查数据库中的键过期时间，并删除过期键。 RDB持久化 AOF持久化 AOF 则以协议文本的方式，将所有对数据库进行过写入的命令（及其参数）记录到 AOF文件，以此达到记录数据库状态的目的。 AOF持久化实现命令追加 在AOF持久化功能处于打开状态时，服务器在执行完一个写命令后，或以协议格式将被执行的写命令追加到服务器的aof——buf缓冲区末尾； struct redisServer{ //.... //aof缓冲区 sds aod_buf; } AOF文件的写入与同步 因为服务器在处理文件事件时可能会执行写命令， 使得一些内容被追加到 aof_buf 缓冲区里面， 所以在服务器每次结束一个事件循环之前， 它都会调用 flushAppendOnlyFile 函数， 考虑是否需要将 aof_buf 缓冲区中的内容写入和保存到 AOF 文件里面， 这个过程可以用以下伪代码表示： def eventLoop(): while True: # 处理文件事件，接收命令请求以及发送命令回复 # 处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中 processFileEvents() # 处理时间事件 processTimeEvents() # 考虑是否要将 aof_buf 中的内容写入和保存到 AOF 文件里面 flushAppendOnlyFile() flushAppendOnlyFile 函数的行为由服务器配置的 appendfsync 选项的值来决定， 各个不同值产生的行为如表 TABLE_APPENDFSYNC 所示。 如果用户没有主动为 appendfsync 选项设置值， 那么 appendfsync 选项的默认值为 everysec ， 关于 appendfsync 选项的更多信息， 请参考 Redis 项目附带的示例配置文件 redis.conf 。 AOF文件载入与数据还原因为AOF文件包含重建数据库状态的所有写命令，所以服务器秩序重新执行AOF文件里保存的写命令。 Redis读取AOF文件并还原数据库状态如下： 创建一个不带网络连接的为客户端 从AOF文件分析并读取一条写命令 使用客户端执行这条写命令 重复2,3直到AOF文件所有写命令被处理 AOF重写 因为AOF持久化是通过保存被执行的写命令来记录数据库状态，所以随着服务器运行时间流逝，AOF文件中内容会越来越多，使用AOF文件来进行数据还原所需的时间越多。实际上ＡＯＦ文件重写是通过读取服务器当前数据库状态来实现的，而不是对现有的ＡＯＦ文件进行读取分析写入。这样能将对一个键的多个写命令替换为一个写命令。 因为AOF重写函数会进行大量的写入操作，如果调用这个函数会长时间阻塞，所以redis将aof重写程序放入到子进程执行。使用子进程同时会带来一个问题：在子进程进行AOF重写期间服务器继续处理了写请求，会导致服务器当前状态与重写后的AOF文件保存的数据库状态不一致。 为了解决这个问题，redis服务器设置了一个AOF重写缓冲区，当redis服务器在重写aof阶段，执行完一个写命令会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区。在子进程完成AOF重写工作后，父进程会调用程序将AOF重写缓冲区中内容写入到新AOF文件中。 Redis设计与实现]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程池]]></title>
    <url>%2F2019%2F05%2F20%2Fjava%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[线程池的基本使用java中实现线程池的类为java.uitl.concurrent.ThreadPoolExecutor继承了AbstractExecutorService类。AbstractExecutorService类基本方法： AbstractExecutorService是一个抽象类，它实现了ExecutorService接口。ExecutorService接口基本方法： ExecutorService又是继承了Executor接口。我们看一下Executor接口的实现： public interface Executor { void execute(Runnable command); } 线程池的创建ThreadPoolExecutor的四个构造函数： public class ThreadPoolExecutor extends AbstractExecutorService { ..... public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,RejectedExecutionHandler handler); public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory,RejectedExecutionHandler handler); ... } 每个参数的含义： corePoolSize（线程池的基本大小）：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时就不再创建。如果调用了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有基本线程。 runnableTaskQueue（任务队列）：用于保存等待执行的任务的阻塞队列。可以选择以下几个阻塞队列。 ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按FIFO（先进先出）原则对元素进行排序。 LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按FIFO排序元素，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列。 SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于Linked-BlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个队列。 PriorityBlockingQueue：一个具有优先级的无限阻塞队列。 maximumPoolSize（线程池最大数量）：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。值得注意的是，如果使用了无界的任务队列这个参数就没什么效果。 ThreadFactory：用于设置创建线程的工厂，可以通过线程工厂给每个创建出来的线程设置更有意义的名字。使用开源框架guava提供的ThreadFactoryBuilder可以快速给线程池里的线程设置有意义的名字，代码如下。 new ThreadFactoryBuilder().setNameFormat(&quot;XX-task-%d&quot;).build(); RejectedExecutionHandler（饱和策略）：当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是AbortPolicy，表示无法处理新任务时抛出异常。在JDK 1.5中Java线程池框架提供了以下4种策略。 AbortPolicy：直接抛出异常。 CallerRunsPolicy：只用调用者所在线程来运行任务 DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 DiscardPolicy：不处理，丢弃掉。 keepAliveTime（线程活动保持时间）：线程池的工作线程空闲后，保持存活的时间。所以，如果任务很多，并且每个任务执行的时间比较短，可以调大时间，提高线程的利用率。 TimeUnit（线程活动保持时间的单位）：可选的单位有天（DAYS）、小时（HOURS）、分钟（MINUTES）、毫秒（MILLISECONDS）、微秒（MICROSECONDS，千分之一毫秒）和纳秒（NANOSECONDS，千分之一微秒）。 在java doc中，并不提倡我们直接使用ThreadPoolExecutor，而是使用Executors类中提供的几个静态方法来创建线程池： Executors.newCachedThreadPool(); //创建一个缓冲池，缓冲池容量大小Integer.MAX_VALUE Executors.newSingleThreadExecutor(); //创建容量为1的缓冲池 Executors.newFixedThreadPool(int); //创建固定容量大小的缓冲池 从它们的具体实现来看，它们实际上也是调用了ThreadPoolExecutor，只不过参数都已配置好了。 //newFixedThreadPool创建的线程池corePoolSize和maximumPoolSize值是相等的，它使用的LinkedBlockingQueue； public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); } //newSingleThreadExecutor将corePoolSize和maximumPoolSize都设置为1，也使用的LinkedBlockingQueue； public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); } //newCachedThreadPool将corePoolSize设置为0，将maximumPoolSize设置为Integer.MAX_VALUE，使用的SynchronousQueue，也就是说来了任务就创建线程运行，当线程空闲超过60秒，就销毁线程。 public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); } 线程的初始化、提交任务、容量的动态调整和关闭线程池默认情况下，创建线程池之后，线程池中是没有线程的，需要提交任务之后才会创建线程。在实际中如果需要线程池创建之后立即创建线程，可以通过以下两个方法办到： prestartCoreThread()：初始化一个核心线程； prestartAllCoreThreads()：初始化所有核心线程 向线程池提交任务有两个方法：execute和submit。 execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。execute()方法实际上是Executor中声明的方法，在ThreadPoolExecutor进行了具体的实现。 submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。submit()方法是在ExecutorService中声明的方法，在AbstractExecutorService就已经有了具体的实现，在ThreadPoolExecutor中并没有对其进行重写。 ThreadPoolExecutor提供了动态调整线程池容量大小的方法： setCorePoolSize()设置核心池大小 setMaximumPoolSize()，设置线程池最大能创建的线程数目大小 当上述参数从小变大时，ThreadPoolExecutor进行线程赋值，还可能立即创建新的线程来执行任务。关闭线程池有shutdown和shutdownNow两个方法。 简单使用范例： public class Test { public static void main(String[] args) { ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(5)); for(int i=0;i&lt;15;i++){ MyTask myTask = new MyTask(i); executor.execute(myTask); System.out.println(&quot;线程池中线程数目：&quot;+executor.getPoolSize()+&quot;，队列中等待执行的任务数目：&quot;+ executor.getQueue().size()+&quot;，已执行玩别的任务数目：&quot;+executor.getCompletedTaskCount()); } executor.shutdown(); } } class MyTask implements Runnable { private int taskNum; public MyTask(int num) { this.taskNum = num; } @Override public void run() { System.out.println(&quot;正在执行task &quot;+taskNum); try { Thread.currentThread().sleep(4000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;task &quot;+taskNum+&quot;执行完毕&quot;); } } 线程池原理分析当提交一个新任务到线程池时，线程池的处理流程如下。 1）线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的工作线程来执行任务。如果核心线程池里的线程都在执行任务，则进入下个流程。 2）线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这个工作队列里。如果工作队列满了，则进入下个流程。 3）线程池判断线程池的线程是否都处于工作状态。如果没有，则创建一个新的工作线程来执行任务。如果已经满了，则交给饱和策略来处理这个任务 看一下execute的源码： public void execute(Runnable command) { if (command == null) throw new NullPointerException(); // 如果线程数小于基本线程数，则进入addIfUnderCorePoolSize(command)，试图创建线程并执行，当正常是add..返回true，结束，无法执行返回false。 //如果线程数大于基本线程数，进入if或者上面的add返回false if (poolSize &gt;= corePoolSize || !addIfUnderCorePoolSize(command)) { // 如线程数大于等于基本线程数或线程创建失败，则将当前任务放到工作队列中。 if (runState == RUNNING &amp;&amp; workQueue.offer(command)) { if (runState != RUNNING || poolSize == 0) ensureQueuedTaskHandled(command); } // 如果线程池不处于运行中或任务无法放入队列，并且当前线程数量小于最大允许的线程数量， // 则创建一个线程执行任务。 else if (!addIfUnderMaximumPoolSize(command)) // 抛出RejectedExecutionException异常 reject(command); // is shutdown or saturated } } //当线程数低于核心池大小时执行的方法 private boolean addIfUnderCorePoolSize(Runnable firstTask) { Thread t = null; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { if (poolSize &lt; corePoolSize &amp;&amp; runState == RUNNING) t = addThread(firstTask); //创建线程去执行firstTask任务 } finally { mainLock.unlock(); } if (t == null) return false; t.start(); return true; } //用提交的任务创建了一个Worker对象，然后调用线程工厂threadFactory创建了一个新的线程t， //然后将线程t的引用赋值给了Worker对象的成员变量thread， //接着通过workers.add(w)将Worker对象添加到工作集当中。 private Thread addThread(Runnable firstTask) { Worker w = new Worker(firstTask); Thread t = threadFactory.newThread(w); //创建一个线程，执行任务 if (t != null) { w.thread = t; //将创建的线程的引用赋值为w的成员变量 workers.add(w); int nt = ++poolSize; //当前线程数加1 if (nt &gt; largestPoolSize) largestPoolSize = nt; } return t; } //work类的run方法， //首先执行的是通过构造器传进来的任务firstTask， //在调用runTask()执行完firstTask之后，在while循环里面不断通过getTask()去取新的任务来执行，那么去哪里取呢？自然是从任务缓存队列里面去取 public void run() { try { Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) { runTask(task); task = null; } } finally { workerDone(this); } } //getTask是ThreadPoolExecutor类中的方法，并不是Worker类中的方法，下面是getTask方法的实现 //在getTask中，先判断当前线程池状态，如果runState大于SHUTDOWN（即为STOP或者TERMINATED），则直接返回null //如果runState为SHUTDOWN或者RUNNING，则从任务缓存队列取任务。 //如果当前线程池的线程数大于核心池大小corePoolSize或者允许为核心池中的线程设置空闲存活时间，则调用poll(time,timeUnit)来取任务，这个方法会等待一定的时间，如果取不到任务就返回null。 //然后判断取到的任务r是否为null，为null则通过调用workerCanExit()方法来判断当前worker是否可以退出， Runnable getTask() { for (;;) { try { int state = runState; if (state &gt; SHUTDOWN) return null; Runnable r; if (state == SHUTDOWN) // Help drain queue r = workQueue.poll(); else if (poolSize &gt; corePoolSize || allowCoreThreadTimeOut) //如果线程数大于核心池大小或者允许为核心池线程设置空闲时间， //则通过poll取任务，若等待一定的时间取不到任务，则返回null r = workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS); else r = workQueue.take(); if (r != null) return r; if (workerCanExit()) { //如果没取到任务，即r为null，则判断当前的worker是否可以退出 if (runState &gt;= SHUTDOWN) // Wake up others interruptIdleWorkers(); //中断处于空闲状态的worker return null; } // Else retry } catch (InterruptedException ie) { // On interruption, re-check runState } } } //workerCanExit()的实现 //如果线程池处于STOP状态、或者任务队列已为空或者允许为核心池线程设置空闲存活时间并且线程数大于1时，允许worker退出。如果允许worker退出，则调用interruptIdleWorkers()中断处于空闲状态的worker private boolean workerCanExit() { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); boolean canExit; //如果runState大于等于STOP，或者任务缓存队列为空了 //或者 允许为核心池线程设置空闲存活时间并且线程池中的线程数目大于1 try { canExit = runState &gt;= STOP || workQueue.isEmpty() || (allowCoreThreadTimeOut &amp;&amp; poolSize &gt; Math.max(1, corePoolSize)); } finally { mainLock.unlock(); } return canExit; } //interruptIdleWorkers()的实现 //从实现可以看出，它实际上调用的是worker的interruptIfIdle()方法 void interruptIdleWorkers() { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { for (Worker w : workers) //实际上调用的是worker的interruptIfIdle()方法 w.interruptIfIdle(); } finally { mainLock.unlock(); } } //在worker的interruptIfIdle()方法 void interruptIfIdle() { final ReentrantLock runLock = this.runLock; if (runLock.tryLock()) { //注意这里，是调用tryLock()来获取锁的，因为如果当前worker正在执行任务，锁已经被获取了，是无法获取到锁的 //如果成功获取了锁，说明当前worker处于空闲状态 try { if (thread != Thread.currentThread()) · thread.interrupt(); } finally { runLock.unlock(); } } } //addIfUnderMaximumPoolSize方法的实现 //这个方法的实现思想和addIfUnderCorePoolSize方法的实现思想非常相似，唯一的区别在于addIfUnderMaximumPoolSize方法是在线程池中的线程数达到了核心池大小并且往任务队列中添加任务失败的情况下执行的： private boolean addIfUnderMaximumPoolSize(Runnable firstTask) { Thread t = null; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { if (poolSize &lt; maximumPoolSize &amp;&amp; runState == RUNNING) t = addThread(firstTask); } finally { mainLock.unlock(); } if (t == null) return false; t.start(); return true; } Work类实现： private final class Worker implements Runnable { private final ReentrantLock runLock = new ReentrantLock(); private Runnable firstTask; volatile long completedTasks; Thread thread; Worker(Runnable firstTask) { this.firstTask = firstTask; } boolean isActive() { return runLock.isLocked(); } void interruptIfIdle() { final ReentrantLock runLock = this.runLock; if (runLock.tryLock()) { try { if (thread != Thread.currentThread()) thread.interrupt(); } finally { runLock.unlock(); } } } void interruptNow() { thread.interrupt(); } private void runTask(Runnable task) { final ReentrantLock runLock = this.runLock; runLock.lock(); try { if (runState &lt; STOP &amp;&amp; Thread.interrupted() &amp;&amp; runState &gt;= STOP) boolean ran = false; beforeExecute(thread, task); //beforeExecute方法是ThreadPoolExecutor类的一个方法，没有具体实现，用户可以根据 //自己需要重载这个方法和后面的afterExecute方法来进行一些统计信息，比如某个任务的执行时间等 try { task.run(); ran = true; afterExecute(task, null); ++completedTasks; } catch (RuntimeException ex) { if (!ran) afterExecute(task, ex); throw ex; } } finally { runLock.unlock(); } } public void run() { try { Runnable task = firstTask; firstTask = null; while (task != null || (task = getTask()) != null) { runTask(task); task = null; } } finally { workerDone(this); //当任务队列中没有任务时，进行清理工作 } } } 来源自：http://www.cnblogs.com/dolphin0520/p/3932921.html 来源自：java并发编程的艺术]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java多线程</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lock]]></title>
    <url>%2F2019%2F05%2F18%2FJAVAlock%2F</url>
    <content type="text"><![CDATA[LOCK接口锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多个线程同时访问共享资源（但是有些锁可以允许多个线程并发的访问共享资源，比如读写锁）。在Lock接口出现之前，Java程序是靠synchronized关键字实现锁功能的，而Java SE 5之后，并发包中新增了Lock接口（以及相关实现类）用来实现锁功能，它提供了与synchronized关键字类似的同步功能，只是在使用时需要显式地获取和释放锁。虽然它缺少了（通过synchronized块或者方法所提供的）隐式获取释放锁的便捷性，但是却拥有了锁获取与释放的可操作性、可中断的获取锁以及超时获取锁等多种synchronized关键字所不具备的同步特性。 Lock接口有三个实现类，一个是ReentrantLock,另两个是ReentrantReadWriteLock类中的两个静态内部类ReadLock和WriteLock。 与互斥锁定相比，读-写锁定允许对共享数据进行更高级别的并发访问。虽然一次只有一个线程（writer 线程）可以修改共享数据，但在许多情况下，任何数量的线程可以同时读取共享数据（reader 线程）。从理论上讲，与互斥锁定相比，使用读-写锁定所允许的并发性增强将带来更大的性能提高。 在实践中，只有在多处理器上并且只在访问模式适用于共享数据时，才能完全实现并发性增强。——例如，某个最初用数据填充并且之后不经常对其进行修改的 collection，因为经常对其进行搜索（比如搜索某种目录），所以这样的 collection 是使用读-写锁定的理想候选者。 队列同步器队列同步器AbstractQueuedSynchronizer（以下简称同步器），是用来构建锁或者其他同步组件的基础框架，它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作。 同步器是实现锁（也可以是任意同步组件）的关键，在锁的实现中聚合同步器，利用同步器实现锁的语义。可以这样理解二者之间的关系：锁是面向使用者的，它定义了使用者与锁交互的接口（比如可以允许两个线程并行访问），隐藏了实现细节；同步器面向的是锁的实现者，它简化了锁的实现方式，屏蔽了同步状态管理、线程的排队、等待与唤醒等底层操作。 同步器的主要使用方式是继承，子类通过继承同步器并实现它的抽象方法来管理同步状态，在抽象方法的实现过程中免不了要对同步状态进行更改，这时就需要使用同步器提供的3个方法（getState()、setState(int newState)和compareAndSetState(int expect,int update)）来进行操作，因为它们能够保证状态的改变是安全的。子类推荐被定义为自定义同步组件的静态内部类，同步器自身没有实现任何同步接口，它仅仅是定义了若干同步状态获取和释放的方法来供自定义同步组件使用，同步器既可以支持独占式地获取同步状态，也可以支持共享式地获取同步状态，这样就可以方便实现不同类型的同步组件（ReentrantLock、ReentrantReadWriteLock和CountDownLatch等）。 实现一个lock–通过队列同步器同步器AQS的设计是基于模板方法模式的，也就是说，使用者需要继承同步器并重写指定的方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些模板方法将会调用使用者重写的方法。 同步器提供的模板方法基本上分为3类：独占式获取与释放同步状态、共享式获取与释放同步状态和查询同步队列中的等待线程情况。自定义同步组件将使用同步器提供的模板方法来实现自己的同步语义。 下面为自定义的一个lock锁： import java.util.concurrent.TimeUnit; import java.util.concurrent.locks.AbstractQueuedSynchronizer; import java.util.concurrent.locks.Condition; import java.util.concurrent.locks.Lock; public class Mutex implements Lock { // 静态内部类，自定义同步器，随后模板方法会调用这些方法 private static class Sync extends AbstractQueuedSynchronizer { private static final long serialVersionUID = -4387327721959839431L; // 是否处于占用状态 protected boolean isHeldExclusively() { return getState() == 1; } // 当状态为0的时候获取锁 public boolean tryAcquire(int acquires) { assert acquires == 1; // Otherwise unused if (compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } // 释放锁，将状态设置为0 protected boolean tryRelease(int releases) { assert releases == 1; // Otherwise unused if (getState() == 0) throw new IllegalMonitorStateException(); setExclusiveOwnerThread(null); setState(0); return true; } // 返回一个Condition，每个condition都包含了一个condition队列 Condition newCondition() { return new ConditionObject(); } } // 仅需要将操作代理到Sync上即可 private final Sync sync = new Sync(); public void lock() { sync.acquire(1); } public boolean tryLock() { return sync.tryAcquire(1); } public void unlock() { sync.release(1); } public Condition newCondition() { return sync.newCondition(); } public boolean isLocked() { return sync.isHeldExclusively(); } public boolean hasQueuedThreads() { return sync.hasQueuedThreads(); } public void lockInterruptibly() throws InterruptedException { sync.acquireInterruptibly(1); } public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException { return sync.tryAcquireNanos(1, unit.toNanos(timeout)); } } 重入锁（ReentrantLock）重入锁就是该锁支持一个线程对资源的重复可利用。除此之外，该锁还支持获取锁的公平与非公平选择。 ReentrantLock中有3个内部类，分别是Sync、FairSync和NonfairSync。 Sync是一个继承AQS的抽象类，使用独占锁，复写了tryRelease方法。tryAcquire方法由它的两个FairSync(公平锁)和NonfairSync(非公平锁)实现。 ReentrantLock的lock方法使用sync的lock方法，Sync的lock方法是个抽象方法，由公平锁和非公平锁去实现。unlock方法直接使用AQS的release方法。所以说公平锁和非公平锁的释放锁过程是一样的，不一样的是获取锁过程。 先看一下公平锁nonfair的lock方法： final void lock() { // acquire方法内部调用tryAcquire方法 // 公平锁的获取锁方法，对于没有获取到的线程，会按照队列的方式挂起线程 acquire(1); } protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // 公平锁这里多了一个!hasQueuedPredecessors()判断，表示是否有线程在队列里等待的时间比当前线程要长，如果有等待时间更长的线程，那么放弃获取锁 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 该方法与nonfairTryAcquire(int acquires)比较，唯一不同的位置为判断条件多了hasQueuedPredecessors()方法，即加入了同步队列中当前节点是否有前驱节点的判断，如果该方法返回true，则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁之后才能继续获取锁。 非公平锁lock方法： final void lock() { // 非公平锁的获取锁 // 跟公平锁的区别就在这里。直接对状态位state进行cas操作，成功就获取锁，这是一种抢占式的方式。不成功跟公平锁一样进入队列挂起线程 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } // 调用Sync的nonfairTryAcquire方法 protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; } 该方法增加了再次获取同步状态的处理逻辑：通过判断当前线程是否为获取锁的线程来决定获取操作是否成功，如果是获取锁的线程再次请求，则将同步状态值进行增加并返回true，表示获取同步状态成功。 对于unlock方法直接使用AQS的release方法。公平锁和非公平锁的释放锁过程是一样的，不一样的是获取锁过程。 protected final boolean tryRelease(int releases) { int c = getState() - releases; // 释放 if (Thread.currentThread() != getExclusiveOwnerThread()) // 如果当前线程不是独占线程，直接抛出异常 throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { // 由于是可重入锁，需要判断是否全部释放了 free = true; setExclusiveOwnerThread(null); // 全部释放的话直接把独占线程设置为null } setState(c); return free; } // 恢复线程 public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; // 恢复第一个挂起的线程 if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } ReentrantLock的默认构造函数使用的是NonfairSync，如果想使用FairSync，使用带有boolean参数的构造函数，传入true表示FairSync，否则是NonfairSync。 读写锁 ReentrantReadWriteLock读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 使用范例 public class Cache { static Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); static ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); static Lock r = rwl.readLock(); static Lock w = rwl.writeLock(); // 获取一个key对应的value public static final Object get(String key) { r.lock(); try { return map.get(key); } finally { r.unlock(); } } // 设置key对应的value，并返回旧的value public static final Object put(String key, Object value) { w.lock(); try { return map.put(key, value); } finally { w.unlock(); } }// 清空所有的内容 public static final void clear() { w.lock(); try { map.clear(); } finally { w.unlock(); } } } 读写锁实现分析 1 - 读写状态的设计 读写锁同样依赖自定义同步器来实现同步功能，而读写状态就是其同步器的同步状态。回想ReentrantLock中自定义同步器的实现，同步状态表示锁被一个线程重复获取的次数，而读写锁的自定义同步器需要在同步状态（一个整型变量）上维护多个读线程和一个写线程的状态，使得该状态的设计成为读写锁实现的关键。如果在一个整型变量上维护多种状态，就一定需要“按位切割使用”这个变量，读写锁将变量切分成了两个部分，高16位表示读，低16位表示写。 2 - 写锁的获取与释放 ReentrantReadWriteLock的tryAcquire方法 protected final boolean tryAcquire(int acquires) { Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) { // 存在读锁或者当前获取线程不是已经获取写锁的线程 if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(c + acquires); return true; } if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) { return false; } setExclusiveOwnerThread(current); return true; } 写锁的释放与ReentrantLock的释放过程基本类似，每次释放均减少写状态，当写状态为0时表示写锁已被释放，从而等待的读写线程能够继续访问读写锁，同时前次写线程的修改对后续读写线程可见。 3- 读锁的获取的释放 ReentrantReadWriteLock的tryAcquireShared方法 protected final int tryAcquireShared(int unused) { for (;;) { int c = getState(); int nextc = c + (1 &lt;&lt; 16); if (nextc &lt; c) throw new Error(&quot;Maximum lock count exceeded&quot;); if (exclusiveCount(c) != 0 &amp;&amp; owner != Thread.currentThread()) return -1; if (compareAndSetState(c, nextc)) return 1; } } 在tryAcquireShared(int unused)方法中，如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态。如果当前线程获取了写锁或者写锁未被获取，则当前线程（线程安全，依靠CAS保证）增加读状态，成功获取读锁。读锁的每次释放（线程安全的，可能有多个读线程同时释放读锁）均减少读状态，减少的值是（1&lt;&lt;16） Condition接口简单使用： public class BoundedQueue&lt;T&gt; { private Object[] items; // 添加的下标，删除的下标和数组当前 private int addIndex, removeI private Lock lock = new R private Condition notEmpty private Condition notFull public BoundedQueue(int size) items = new Object[si } // 添加一个元素，如果数组满，则添加线 public void add(T t) throws I lock.lock(); try { while (count == items.length) notFull.await(); items[addIndex] = t; if (++addIndex == items.length) addIndex = 0; ++count; notEmpty.signal(); } finally { ock.unlock(); } } // 由头部删除一个元素，如果数组空，则删除线程进入等待状态，直到有新添加 @SuppressWarnings(&quot;unchecked&quot;) public T remove() throws InterruptedException { lock.lock(); try { while (count == 0) notEmpty.await(); Object x = items[removeIndex]; if (++removeIndex == items.length) removeIndex = 0; --count; notFull.signal(); return (T) x; } finally { lock.unlock(); } } } 全文来自 java并发编程的艺术]]></content>
      <categories>
        <category>java</category>
        <category>lock</category>
      </categories>
      <tags>
        <tag>java多线程</tag>
        <tag>队列同步器</tag>
        <tag>lock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[队列同步器]]></title>
    <url>%2F2019%2F05%2F15%2F%E9%98%9F%E5%88%97%E5%90%8C%E6%AD%A5%E5%99%A8%2F</url>
    <content type="text"><![CDATA[队列同步器队列同步器AbstractQueuedSynchronizer（以下简称同步器），是用来构建锁或者其他同步组件的基础框架，它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作。 同步器的主要使用方式是继承，子类通过继承同步器并实现它的抽象方法来管理同步状态，在抽象方法的实现过程中免不了要对同步状态进行更改，这时就需要使用同步器提供的3个方法（getState()、setState(int newState)和compareAndSetState(int expect,int update)）来进行操作，因为它们能够保证状态的改变是安全的。子类推荐被定义为自定义同步组件的静态内部类，同步器自身没有实现任何同步接口，它仅仅是定义了若干同步状态获取和释放的方法来供自定义同步组件使用，同步器既可以支持独占式地获取同步状态，也可以支持共享式地获取同步状态，这样就可以方便实现不同类型的同步组件（ReentrantLock、ReentrantReadWriteLock和CountDownLatch等）。 队列同步器的接口与示例同步器的设计是基于模板方法模式的，也就是说，使用者需要继承同步器并重写指定的方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些模板方法将会调用使用者重写的方法。 以下是重写同步器方法实现的一个独占锁。 class Mutex implements Lock { // 静态内部类，自定义同步器 private static class Sync extends AbstractQueuedSynchronizer { // 是否处于占用状态 protected boolean isHeldExclusively() { return getState() == 1; } // 当状态为0的时候获取锁 public boolean tryAcquire(int acquires) { if (compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } // 释放锁，将状态设置为0 protected boolean tryRelease(int releases) { if (getState() == 0) throw new IllegalMonitorStateException(); setExclusiveOwnerThread(null); setState(0); return true; } // 返回一个Condition，每个condition都包含了一个condition队列 Condition newCondition() { return new ConditionObject(); } } // 仅需要将操作代理到Sync上即可 private final Sync sync = new Sync(); public void lock() { sync.acquire(1); } public boolean tryLock() { return sync.tryAcquire(1); } public void unlock() { sync.release(1); } public Condition newCondition() { return sync.newCondition(); } public boolean isLocked() { return sync.isHeldExclusively(); } public boolean hasQueuedThreads() { return sync.hasQueuedThreads(); } public void lockInterruptibly() throws InterruptedException { sync.acquireInterruptibly(1); } public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException { return sync.tryAcquireNanos(1, unit.toNanos(timeout)); } } 队列同步器的实现分析同步队列同步器依赖内部的同步队列（一个FIFO双向队列）来完成同步状态的管理，当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成为一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点中的线程唤醒，使其再次尝试获取同步状态。 节点是构成同步队列的基础。 同步器拥有首节点（head）和尾节点（tail），没有成功获取同步状态的线程将会成为节点加入该队列的尾部，同步队列的基本结构如图示。 同步队列在加入尾结点需要cas来保证线程安全，而设置头结点不需要。 独占式同步状态获取与释放主要流程为： 通过调用同步器的acquire(int arg)方法可以获取同步状态，该方法对中断不敏感，也就是由于线程获取同步状态失败后进入同步队列中，后续对线程进行中断操作时，线程不会从同步队列中移出。 //同步器的acquire方法 public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 代码主要完成了同步状态获取、节点构造、加入同步队列以及在同步队列中自旋等待的相关工作，其主要逻辑是：首先调用自定义同步器实现的tryAcquire(int arg)方法，该方法保证线程安全的获取同步状态，如果同步状态获取失败，则构造同步节点（独占式Node.EXCLUSIVE，同一时刻只能有一个线程成功获取同步状态）并通过addWaiter(Node node)方法将该节点加入到同步队列的尾部，最后调用acquireQueued(Node node,int arg)方法，使得该节点以“死循环”的方式获取同步状态。如果获取不到则阻塞节点中的线程，而被阻塞线程的唤醒主要依靠前驱节点的出队或阻塞线程被中断来实现。 //同步器的addWaiter和enq方法 private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // 快速尝试在尾部添加 Node pred = tail;if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node; } private Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 当前线程获取同步状态并执行了相应逻辑之后，就需要释放同步状态，使得后续节点能够继续获取同步状态。通过调用同步器的release(int arg)方法可以释放同步状态，该方法在释放了同步状态之后，会唤醒其后继节点（进而使后继节点重新尝试获取同步状态）。 public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } 总结：在获取同步状态时，同步器维护一个同步队列，获取状态失败的线程都会被加入到队列中并在队列中进行自旋；移出队列（或停止自旋）的条件是前驱节点为头节点且成功获取了同步状态。在释放同步状态时，同步器调用tryRelease(int arg)方法释放同步状态，然后唤醒头节点的后继节点。 共享式同步状态获取与释放共享式获取与独占式获取最主要的区别在于同一时刻能否有多个线程同时获取到同步状态。 通过调用同步器的acquireShared(int arg)方法可以共享式地获取同步状态。 public final void acquireShared(int arg) { if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg); } private void doAcquireShared(int arg) { final Node node = addWaiter(Node.SHARED); boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head) { int r = tryAcquireShared(arg); if (r &gt;= 0) { setHeadAndPropagate(node, r); p.next = null; if (interrupted)selfInterrupt(); failed = false; return; } } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 在acquireShared(int arg)方法中，同步器调用tryAcquireShared(int arg)方法尝试获取同步状态，tryAcquireShared(int arg)方法返回值为int类型，当返回值大于等于0时，表示能够获取到同步状态。因此，在共享式获取的自旋过程中，成功获取到同步状态并退出自旋的条件就是tryAcquireShared(int arg)方法返回值大于等于0。可以看到，在doAcquireShared(int arg)方法的自旋过程中，如果当前节点的前驱为头节点时，尝试获取同步状态，如果返回值大于等于0，表示该次获取同步状态成功并从自旋过程中退出。 与独占式一样，共享式获取也需要释放同步状态，通过调用releaseShared(int arg)方法可以释放同步状态。方法在释放同步状态之后，将会唤醒后续处于等待状态的节点。对于能够支持多个线程同时访问的并发组件（比如Semaphore），它和独占式主要区别在于tryReleaseShared(int arg)方法必须确保同步状态（或者资源数）线程安全释放，一般是通过循环和CAS来保证的，因为释放同步状态的操作会同时来自多个线程。 public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } 独占式超时获取同步状态通过调用同步器的doAcquireNanos(int arg,long nanosTimeout)方法可以超时获取同步状态，即在指定的时间段内获取同步状态，如果获取到同步状态则返回true，否则，返回false。该方法提供了传统Java同步操作（比如synchronized关键字）所不具备的特性。 超时获取同步状态过程可以被视作响应中断获取同步状态过程的“增强版”，doAcquireNanos(int arg,long nanosTimeout)方法在支持响应中断的基础上，增加了超时获取的特性。针对超时获取，主要需要计算出需要睡眠的时间间隔nanosTimeout，为了防止过早通知，nanosTimeout计算公式为：nanosTimeout-=now-lastTime，其中now为当前唤醒时间，lastTime为上次唤醒时间，如果nanosTimeout大于0则表示超时时间未到，需要继续睡眠nanosTimeout纳秒，反之，表示已经超时。 private boolean doAcquireNanos(int arg, long nanosTimeout)throws InterruptedException { long lastTime = System.nanoTime(); final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try { for (;;) { final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return true; } if (nanosTimeout &lt;= 0) return false; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); long now = System.nanoTime(); //计算时间，当前时间now减去睡眠之前的时间lastTime得到已经睡眠 //的时间delta，然后被原有超时时间nanosTimeout减去，得到了 //还应该睡眠的时间 nanosTimeout -= now - lastTime; lastTime = now; if (Thread.interrupted()) throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); } } 该方法在自旋过程中，当节点的前驱节点为头节点时尝试获取同步状态，如果获取成功则从该方法返回，这个过程和独占式同步获取的过程类似，但是在同步状态获取失败的处理上有所不同。如果当前线程获取同步状态失败，则判断是否超时（nanosTimeout小于等于0表示已经超时），如果没有超时，重新计算超时间隔nanosTimeout，然后使当前线程等待nanosTimeout纳秒（当已到设置的超时时间，该线程会从LockSupport.parkNanos(Objectblocker,long nanos)方法返回）。 如果nanosTimeout小于等于spinForTimeoutThreshold（1000纳秒）时，将不会使该线程进行超时等待，而是进入快速的自旋过程。原因在于，非常短的超时等待无法做到十分精确，如果这时再进行超时等待，相反会让nanosTimeout的超时从整体上表现得反而不精确。因此，在超时非常短的场景下，同步器会进入无条件的快速自旋。 独占式超时获取同步态的流程如图5所示。 全文来自 Java并发编程的艺术]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java多线程</tag>
        <tag>队列同步器</tag>
        <tag>AQS</tag>
        <tag>lock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr简单使用]]></title>
    <url>%2F2019%2F04%2F02%2Fsolr%2F</url>
    <content type="text"><![CDATA[solr的安装与配置 solr简介 Solr（读作“solar”）是Apache Lucene项目的开源企业搜索平台。其主要功能包括全文检索、命中标示[1]、分面搜索、动态聚类、数据库集成，以及富文本（如Word、PDF）的处理。Solr是高度可扩展的，并提供了分布式搜索和索引复制。Solr是最流行的企业级搜索引擎，[2]Solr 4还增加了NoSQL支持。[3] Solr是用Java编写、运行在Servlet容器（如Apache Tomcat或Jetty）的一个独立的全文搜索服务器。 Solr采用了Lucene Java搜索库为核心的全文索引和搜索，并具有类似REST的HTTP/XML和JSON的API。 Solr强大的外部配置功能使得无需进行Java编码，便可对其进行调整以适应多种类型的应用程序。Solr有一个插件架构，以支持更多的高级定制。 solr的安装 下载solr 8.0版并解压 ，下载地址为： https://www-eu.apache.org/dist/lucene/solr/8.0.0/ 下载并安装tomcat 将解压后solr-8.0.0\server\solr-webapp\webapp这个文件夹复制到tomcat的apache-tomcat-9.0.14\webapps文件夹下，并改名为solr（为了方便访问） 把solr下example/lib/ext 目录下的所有的 jar 包，添加到 solr 的工程中(\WEB-INF\lib目录下)。 在任意位置创建solr-home 目录如D:\solrhome，然后将solr-8.0.0\server\solr文件夹复制到该目录下 创建collection：在solr目录下创建文件夹firstcore（也就是集合），将\solr\configsets\sample_techproducts_configs下的conf文件复制到该目录下 关联 solr 及 solrhome。需要修改 solr 工程的 web.xml 文件。 &lt;env-entry&gt; &lt;env-entry-name&gt;solr/home&lt;/env-entry-name&gt; &lt;env-entry-value&gt;d:\solrhome&lt;/env-entry-value&gt; &lt;env-entry-type&gt;java.lang.String&lt;/env-entry-type&gt; &lt;/env-entry&gt; 启动tomcat apache-tomcat-9.0.14\bin\startup.bat 配置中文分析器IK Analyzer下载IK Analyzer： 因为solr为8.0版本，网上常见的IK Analyzer版本为IKAnalyzer2012FF_u1.jar无法使用，需要下载最新版本，下载网址为：https://github.com/magese/ik-analyzer-solr7 在solr中配置IK Analyzer 将resources目录下的5个配置文件放入的solr的服务Jetty或Tomcat的webapp/solr/WEB-INF/classes/目录下; ①IKAnalyzer.cfg.xml ②ext.dic ③stopword.dic ④ik.conf ⑤dynamicdic.txt 配置的Solr的managed-schema(路径为solr\firstCore\conf)，添加ik分词器，示例如下; &lt;！ - ik分词器 - &gt; &lt;fieldType name =“text_ik”class =“solr.TextField”&gt; &lt;analyzer type =“index”&gt; &lt;tokenizer class =“org.wltea.analyzer.lucene.IKTokenizerFactory”useSmart = “false”conf =“ik.conf”/&gt; &lt;filter class =“solr.LowerCaseFilterFactory”/&gt; &lt;/ analyzer&gt; &lt;analyzer type =“query”&gt; &lt;tokenizer class =“org.wltea.analyzer.lucene.IKTokenizerFactory” useSmart =“true”conf =“ik.conf”/&gt; &lt;filter class =“solr.LowerCaseFilterFactory”/&gt; &lt;/ analyzer&gt; &lt;/ fieldType&gt; 配置域 域相当于数据库的表字段，用户存放数据，因此用户根据业务需要去定义相关的Field（域），一般来说，每一种对应着一种数据，用户对同一种数据进行相同的操作。 域的常用属性： • name：指定域的名称 • type：指定域的类型 • indexed：是否索引 • stored：是否存储 • required：是否必须 • multiValued：是否多值 域修改solrhome的schema.xml文件,添加field &lt;field name=&quot;item_title&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; 复制域复制域的作用在于将某一个Field中的数据复制到另一个域中(可以用于多字段搜索) &lt;field name=&quot;item_keywords&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;false&quot; multiValued=&quot;true&quot;/&gt; &lt;copyField source=&quot;item_title&quot; dest=&quot;item_keywords&quot;/&gt; &lt;copyField source=&quot;item_name&quot; dest=&quot;item_keywords&quot;/&gt; 动态域当我们需要动态扩充字段时，我们需要使用动态域。需要实现的效果如下： &lt;dynamicField name=&quot;item_spec_*&quot; type=&quot;string&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt; Spring Data Solr的简单使用入门demo 创建maven工程，pom.xml中引入依赖 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-solr&lt;/artifactId&gt; &lt;version&gt;1.5.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.9&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 在src/main/resources下创建 applicationContext-solr.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:solr=&quot;http://www.springframework.org/schema/data/solr&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/data/solr http://www.springframework.org/schema/data/solr/spring-solr-1.0.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- solr服务器地址,记得加上集合名 --&gt; &lt;solr:solr-server id=&quot;solrServer&quot; url=&quot;http://127.0.0.1:8080/solr/firstcore&quot; /&gt; &lt;!-- solr模板，使用solr模板可对索引库进行CRUD的操作 --&gt; &lt;bean id=&quot;solrTemplate&quot; class=&quot;org.springframework.data.solr.core.SolrTemplate&quot;&gt; &lt;constructor-arg ref=&quot;solrServer&quot; /&gt; &lt;/bean&gt; &lt;/beans&gt; 将需要导入solr的实体类添加注解@Field 如果属性与配置文件定义的域名称不一致，需要在注解中指定域名称。如同下面的title，在solr的配置文件中名为item_title, 所以要加上注解@Field(&quot;item_titel&quot;),其他也一样。（） public class TbItem implements Serializable{ @Field private Long id; @Field(&quot;item_title&quot;) private String title; @Field(&quot;item_price&quot;) private BigDecimal price; } 创建测试类TestTemplate.java @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations=&quot;classpath:applicationContext-solr.xml&quot;) public class TestTemplate { @Autowired private SolrTemplate solrTemplate; //增加到solr索引库 @Test public void testAdd(){ TbItem item=new TbItem(); item.setId(1L); item.setTitle(&quot;barktegh&quot;); item.setPrice(new BigDecimal(2000)); solrTemplate.saveBean(item); solrTemplate.commit(); } //按主键查询 @Test public void testFindOne(){ TbItem item = solrTemplate.getById(1, TbItem.class); System.out.println(item.getTitle()); } //循环项solr插入数据 @Test public void testAddList(){ List&lt;TbItem&gt; list=new ArrayList(); for(int i=0;i&lt;100;i++){ TbItem item=new TbItem(); item.setId(i+1L); item.setTitle(&quot;华为Mate&quot;+i); item.setPrice(new BigDecimal(2000+i)); list.add(item); } solrTemplate.saveBeans(list); solrTemplate.commit(); } //分页查询 @Test public void testPageQuery(){ Query query=new SimpleQuery(&quot;*:*&quot;); query.setOffset(20);//开始索引（默认0） query.setRows(20);//每页记录数(默认10) ScoredPage&lt;TbItem&gt; page = solrTemplate.queryForPage(query, TbItem.class); System.out.println(&quot;总记录数：&quot;+page.getTotalElements()); List&lt;TbItem&gt; list = page.getContent(); showList(list); } //条件查询 @Test public void testPageQueryMutil(){ Query query=new SimpleQuery(&quot;*:*&quot;); Criteria criteria=new Criteria(&quot;item_title&quot;).contains(&quot;2&quot;); criteria=criteria.and(&quot;item_title&quot;).contains(&quot;5&quot;); query.addCriteria(criteria); //query.setOffset(20);//开始索引（默认0） //query.setRows(20);//每页记录数(默认10) ScoredPage&lt;TbItem&gt; page = solrTemplate.queryForPage(query, TbItem.class); System.out.println(&quot;总记录数：&quot;+page.getTotalElements()); List&lt;TbItem&gt; list = page.getContent(); showList(list); } //删除全部数据 @Test public void testDeleteAll(){ Query query=new SimpleQuery(&quot;*:*&quot;); solrTemplate.delete(query); solrTemplate.commit(); } //显示记录数据 private void showList(List&lt;TbItem&gt; list){ for(TbItem item:list){ System.out.println(item.getTitle() +item.getPrice()); } } } 在 http://127.0.0.1:8080/solr 可以查询到数据变化]]></content>
      <categories>
        <category>solr</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>solr</tag>
        <tag>spring-data-solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap详解]]></title>
    <url>%2F2019%2F03%2F02%2FhashMap%2F</url>
    <content type="text"><![CDATA[HashMap介绍 前言HashMap是一个经典的key-value结构，是线程不安全的。如果要使用线程安全的hashMap可以使用并发包里的ConcurrentHashMap。jdk在1.7和1.8的具体实现稍有不同。 HaspMap在1.7的实现基本数据结构和变量： hashmap的内部数据结构是一个Entry数组transient Node&lt;K,V&gt;[] table（被关键citransient修饰是为了序列化时只用序列化已使用的数据）。 其中Entry是一个内部类，源码如下： static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Entry&lt;K,V&gt; next; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + &quot;=&quot; + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; } return false; } } Entry主要有四个成员变量： key就是键 value 是值 hash存放当前key的hashcode next用于实现链表 HashMap还有一些核心变量如下： static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; 初始化桶的大小，默认为16。选择2的n次方是为了在扩容是计算hashcode将取模运算转为位运算。 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); } static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; 桶最大值 static final float DEFAULT_LOAD_FACTOR = 0.75f; 默认装载因子0.75 transient int size; Map存放数量的大小 int threshold; 桶大小，可在初始化时显式指定 final float loadFactor; 装载因子，可在初始化时显式指定。 其中当map的数量达到threshold*loadFactor时，就需要对map就行扩容。 put方法HashMap存放数据1.7源码如下： public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null; } 流程如下： 判断当前数组是否需要初始化。 如果 key 为空，则 put 一个空值进去。 根据 key 计算出 hashcode。 根据计算出的 hashcode 定位出所在桶。 如果桶是一个链表则需要遍历判断里面的 hashcode、key 是否和传入 key 相等，如果相等则进行覆盖，并返回原来的值。 如果桶是空的或者没有找到key，说明当前位置没有数据存入；新增一个 Entry 对象写入当前位置。 void addEntry(int hash, K key, V value, int bucketIndex) { if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) { resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex); } void createEntry(int hash, K key, V value, int bucketIndex) { Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++; } get方法HashMap获取数据1.7源码如下： public V get(Object key) { if (key == null) return getForNullKey(); Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue(); } final Entry&lt;K,V&gt; getEntry(Object key) { if (size == 0) { return null; } int hash = (key == null) ? 0 : hash(key); for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } return null; } 流程如下： 根据key计算出hashcode，定位到桶的位置 如果桶为空，直接返回null 不然遍历该桶，比较key，value，hash值是否相等；如果相等就返回值，不然返回null HaspMap在1.8的实现1.8相比较1.7的改变主要在以下几个点： 将Entry更改为Node 在链表长度超过8时就更改为红黑树，加快查询。 添加static final int TREEIFY_THRESHOLD = 8变量。 链表头插法改为尾插法（保持原来的顺序），就要为了解决并发put导致resize出现死循环。 以上这些改变都是为了解决hash冲突时链表长度过长，导致查询效率下降的问题，通过将长度超过8的链表转为红黑树来加快查询。此时的数据结构如下： put方法HashMap存放数据1.8源码： public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } /** * Implements Map.put and related methods. * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don&apos;t change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; } 流程如下： 判断桶数组是否为空；如果为空通过resize()来初始化桶数组。 否则通过hash定位到桶位置；若果桶为空，新建一个Node节点（或者说新桶）。 否则先判断当前桶的hash，key与写入的hash，key是否相等；相等的话将值赋予给e； 如果当前桶是红黑树，就以红黑树的方式写入； 如果是个链表，如果在遍历过程中找到 key 相同时直接退出遍历。 如果没有找到相同的节点，就需要将当前的 key、value 封装成一个新节点写入到当前桶的后面（形成链表）赋予给e，接着判断当前链表的大小是否大于预设的阈值，大于时就要转换为红黑树。 判断，如果 e != null 就相当于存在相同的 key,那就需要将值覆盖。 最后判断是否需要进行扩容。 get方法HashMap获取数据1.8源码如下： public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; } /** * Implements Map.get and related methods. * * @param hash hash for key * @param key the key * @return the node, or null if none */ final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } 流程如下： 首先将 key hash 之后取得所定位的桶。 如果桶为空则直接返回 null 。 否则判断桶的第一个位置(有可能是链表、红黑树)的 key 是否为查询的 key，是就直接返回 value。 如果第一个不匹配，则判断它的下一个是红黑树还是链表。 红黑树就按照树的查找方式返回值。 不然就按照链表的方式遍历匹配返回值。 ConcurrentHashMap ConcurrentHashMap在1.7中的实现基本数据结构 ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock，不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理。无论是读操作还是写操作都能保证很高的性能：在进行读操作时(几乎)不需要加锁，而在写操作时通过锁分段技术只对所操作的段加锁而不影响客户端对其它段的访问。特别地，在理想状态下，ConcurrentHashMap 可以支持 16 个线程执行并发写操作（如果并发级别设为16），及任意数量线程的读操作。 ConcurrentHashMap的高效并发机制是通过以下三方面来保证的： 通过锁分段技术保证并发环境下的写操作； 通过 HashEntry的不变性、Volatile变量的内存可见性和加锁重读机制保证高效、安全的读操作； 通过不加锁和加锁两种方案控制跨段操作的的安全性。 主要变量如下： /** * Mask value for indexing into segments. The upper bits of a * key&apos;s hash code are used to choose the segment. */ final int segmentMask; // 用于定位段，大小等于segments数组的大小减 1，是不可变的 /** * Shift value for indexing within segments. */ final int segmentShift; // 用于定位段，大小等于32(hash值的位数)减去对segments的大小取以2为底的对数值，是不可变的 /** * The segments, each of which is a specialized hash table */ final Segment&lt;K,V&gt;[] segments; // ConcurrentHashMap的底层结构是一个Segment数组 Segment类主要组成： // static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable { /** * The number of elements in this segment&apos;s region. */ transient volatile int count; // Segment中元素的数量，可见的 /** * Number of updates that alter the size of the table. This is * used during bulk-read methods to make sure they see a * consistent snapshot: If modCounts change during a traversal * of segments computing size or checking containsValue, then * we might have an inconsistent view of state so (usually) * must retry. */ transient int modCount; //对count的大小造成影响的操作的次数（比如put或者remove操作） /** * The table is rehashed when its size exceeds this threshold. * (The value of this field is always &lt;tt&gt;(int)(capacity * * loadFactor)&lt;/tt&gt;.) */ transient int threshold; // 阈值，段中元素的数量超过这个值就会对Segment进行扩容 /** * The per-segment table. */ transient volatile HashEntry&lt;K,V&gt;[] table; // 链表数组 /** * The load factor for the hash table. Even though this value * is same for all segments, it is replicated to avoid needing * links to outer object. * @serial */ final float loadFactor; // 段的负载因子，其值等同于ConcurrentHashMap的负载因子 ... } HashEntry类 /** * HashMap 中的 Entry 类 */ static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final K key; V value; Entry&lt;K,V&gt; next; final int hash; /** * Creates new entry. */ Entry(int h, K k, V v, Entry&lt;K,V&gt; n) { value = v; next = n; key = k; hash = h; } ... } put方法public V put(K key, V value) { Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); int hash = hash(key); int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); return s.put(key, hash, value, false); } final V put(K key, int hash, V value, boolean onlyIfAbsent) { HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try { HashEntry&lt;K,V&gt;[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V&gt; first = entryAt(tab, index); for (HashEntry&lt;K,V&gt; e = first;;) { if (e != null) { K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { e.value = value; ++modCount; } break; } e = e.next; } else { if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); else setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; } } } finally { unlock(); } return oldValue; } 流程如下： 根据key的hash值定位到Segment 利用 scanAndLockForPut() 获得Segment的锁 尝试自旋获取锁。 如果重试的次数达到了 MAX_SCAN_RETRIES 则改为阻塞锁获取，保证能获取成功。 将当前 Segment 中的 table 通过 key 的 hashcode 定位到 HashEntry。 遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧的 value。不为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会先判断是否需要扩容。 最后会解除所获取当前 Segment 的锁。 get方法public V get(Object key) { Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) { for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) { K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; } } return null; } ConcurrentHashMap在1.8中的实现1.8相较于1.7更改主要在一下几个点： 抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性 存放数据的 HashEntry 改为 Node，但作用都是相同的 val next 都用了 volatile 修饰，保证了可见性 链表长度超过8就采用红黑树来实现 主要引用： HashMap? ConcurrentHashMap? 相信看完这篇没人能难住你 Map 综述（三）：彻头彻尾理解 ConcurrentHashMap]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stanfordCoreNLP的使用方式]]></title>
    <url>%2F2019%2F01%2F21%2FstanfordCoreNLP%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[#使用Standford Core NLP的几种方法 stanford corenlp 2018-02-27java环境： 1.8 64位 因为斯坦福corenlp最新版都推荐使用服务器来运行，且nltk包不在支持，直接调用会出现一些问题，建议使用服务器来使用 ##1 使用Core nlp服务器，在来连接服务器 创建Core nlp服务器 在命令行中进入corenlp文件所在位置，输入开启服务器命令 e: cd E:\corenlp\stanford-corenlp-full-2018-02-27 java -mx4g -cp &quot;*&quot; edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 使用nltk（3.3v以上）包连接服务器 API网址：https://www.nltk.org/_modules/nltk/parse/corenlp.html &gt;&gt;&gt; from nltk.parse import CoreNLPParser # Lexical Parser &gt;&gt;&gt; parser = CoreNLPParser(url=&apos;http://localhost:9000&apos;) # Parse tokenized text. &gt;&gt;&gt; list(parser.parse(&apos;What is the airspeed of an unladen swallow ?&apos;.split())) [Tree(&apos;ROOT&apos;, [Tree(&apos;SBARQ&apos;, [Tree(&apos;WHNP&apos;, [Tree(&apos;WP&apos;, [&apos;What&apos;])]), Tree(&apos;SQ&apos;, [Tree(&apos;VBZ&apos;, [&apos;is&apos;]), Tree(&apos;NP&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;DT&apos;, [&apos;the&apos;]), Tree(&apos;NN&apos;, [&apos;airspeed&apos;])]), Tree(&apos;PP&apos;, [Tree(&apos;IN&apos;, [&apos;of&apos;]), Tree(&apos;NP&apos;, [Tree(&apos;DT&apos;, [&apos;an&apos;]), Tree(&apos;JJ&apos;, [&apos;unladen&apos;])])]), Tree(&apos;S&apos;, [Tree(&apos;VP&apos;, [Tree(&apos;VB&apos;, [&apos;swallow&apos;])])])])]), Tree(&apos;.&apos;, [&apos;?&apos;])])])] # Parse raw string. &gt;&gt;&gt; list(parser.raw_parse(&apos;What is the airspeed of an unladen swallow ?&apos;)) [Tree(&apos;ROOT&apos;, [Tree(&apos;SBARQ&apos;, [Tree(&apos;WHNP&apos;, [Tree(&apos;WP&apos;, [&apos;What&apos;])]), Tree(&apos;SQ&apos;, [Tree(&apos;VBZ&apos;, [&apos;is&apos;]), Tree(&apos;NP&apos;, [Tree(&apos;NP&apos;, [Tree(&apos;DT&apos;, [&apos;the&apos;]), Tree(&apos;NN&apos;, [&apos;airspeed&apos;])]), Tree(&apos;PP&apos;, [Tree(&apos;IN&apos;, [&apos;of&apos;]), Tree(&apos;NP&apos;, [Tree(&apos;DT&apos;, [&apos;an&apos;]), Tree(&apos;JJ&apos;, [&apos;unladen&apos;])])]), Tree(&apos;S&apos;, [Tree(&apos;VP&apos;, [Tree(&apos;VB&apos;, [&apos;swallow&apos;])])])])]), Tree(&apos;.&apos;, [&apos;?&apos;])])])] # Neural Dependency Parser &gt;&gt;&gt; from nltk.parse.corenlp import CoreNLPDependencyParser &gt;&gt;&gt; dep_parser = CoreNLPDependencyParser(url=&apos;http://localhost:9000&apos;) &gt;&gt;&gt; parses = dep_parser.parse(&apos;What is the airspeed of an unladen swallow ?&apos;.split()) &gt;&gt;&gt; [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses] [[((&apos;What&apos;, &apos;WP&apos;), &apos;cop&apos;, (&apos;is&apos;, &apos;VBZ&apos;)), ((&apos;What&apos;, &apos;WP&apos;), &apos;nsubj&apos;, (&apos;airspeed&apos;, &apos;NN&apos;)), ((&apos;airspeed&apos;, &apos;NN&apos;), &apos;det&apos;, (&apos;the&apos;, &apos;DT&apos;)), ((&apos;airspeed&apos;, &apos;NN&apos;), &apos;nmod&apos;, (&apos;swallow&apos;, &apos;VB&apos;)), ((&apos;swallow&apos;, &apos;VB&apos;), &apos;case&apos;, (&apos;of&apos;, &apos;IN&apos;)), ((&apos;swallow&apos;, &apos;VB&apos;), &apos;det&apos;, (&apos;an&apos;, &apos;DT&apos;)), ((&apos;swallow&apos;, &apos;VB&apos;), &apos;amod&apos;, (&apos;unladen&apos;, &apos;JJ&apos;)), ((&apos;What&apos;, &apos;WP&apos;), &apos;punct&apos;, (&apos;?&apos;, &apos;.&apos;))]] # Tokenizer 分词 &gt;&gt;&gt; parser = CoreNLPParser(url=&apos;http://localhost:9000&apos;) &gt;&gt;&gt; list(parser.tokenize(&apos;What is the airspeed of an unladen swallow?&apos;)) [&apos;What&apos;, &apos;is&apos;, &apos;the&apos;, &apos;airspeed&apos;, &apos;of&apos;, &apos;an&apos;, &apos;unladen&apos;, &apos;swallow&apos;, &apos;?&apos;] # POS Tagger 词性标注 &gt;&gt;&gt; pos_tagger = CoreNLPParser(url=&apos;http://localhost:9000&apos;, tagtype=&apos;pos&apos;) &gt;&gt;&gt; list(pos_tagger.tag(&apos;What is the airspeed of an unladen swallow ?&apos;.split())) [(&apos;What&apos;, &apos;WP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;airspeed&apos;, &apos;NN&apos;), (&apos;of&apos;, &apos;IN&apos;), (&apos;an&apos;, &apos;DT&apos;), (&apos;unladen&apos;, &apos;JJ&apos;), (&apos;swallow&apos;, &apos;VB&apos;), (&apos;?&apos;, &apos;.&apos;)] # NER Tagger 命名实体识别 &gt;&gt;&gt; ner_tagger = CoreNLPParser(url=&apos;http://localhost:9000&apos;, tagtype=&apos;ner&apos;) &gt;&gt;&gt; list(ner_tagger.tag((&apos;Rami Eid is studying at Stony Brook University in NY&apos;.split()))) [(&apos;Rami&apos;, &apos;PERSON&apos;), (&apos;Eid&apos;, &apos;PERSON&apos;), (&apos;is&apos;, &apos;O&apos;), (&apos;studying&apos;, &apos;O&apos;), (&apos;at&apos;, &apos;O&apos;), (&apos;Stony&apos;, &apos;ORGANIZATION&apos;), (&apos;Brook&apos;, &apos;ORGANIZATION&apos;), (&apos;University&apos;, &apos;ORGANIZATION&apos;), (&apos;in&apos;, &apos;O&apos;), (&apos;NY&apos;, &apos;STATE_OR_PROVINCE&apos;)] 或者使用from stanfordcorenlp import StanfordCoreNLP，网址为https://github.com/Lynten/stanford-corenlp nlp = StanfordCoreNLP(&apos;http://localhost&apos;, port=9000) # 常用api print &apos;Tokenize:&apos;, nlp.word_tokenize(sentence) print &apos;Part of Speech:&apos;, nlp.pos_tag(sentence) print &apos;Named Entities:&apos;, nlp.ner(sentence) print &apos;Constituency Parsing:&apos;, nlp.parse(sentence) print &apos;Dependency Parsing:&apos;, nlp.dependency_parse(sentence) # 一般的 # annotators: tokenize, ssplit, pos, lemma, ner, parse, depparse, dcoref (See Detail) # pipelineLanguage: en, zh, ar, fr, de, es (English, Chinese, Arabic, French, German, Spanish) (See Annotator Support Detail) # outputFormat: json, xml, text props = {&apos;annotators&apos;: &apos;tokenize, ssplit, ner, depparse&apos;, &apos;pipelineLanguage&apos;: &apos;en&apos;, &apos;outputFormat&apos;: &apos;json&apos;} ParseResult = nlp.annotate(raw_sent, properties=props) nlp.close（）＃别忘了关闭！后端服务器将消耗大量的memery。 2 不使用服务器（不推荐，以不更新）使用nltk直接调用包2018-12-28测试发现侧方法会出现utf8解析问题，暂时无法解决，推荐使用上面的方法。 # java路径，需要修改 java_path = &quot;C:\\Program Files\\Java\\jre1.8.0_191\\bin\\java.exe&quot; os.environ[&apos;JAVA_HOME&apos;] = java_path os.environ[&quot;STANFORD_PARSER&quot;] = Utils.get_project_path()+&quot;\\lib_data\\stanford-parser-full-2018-02-27\\stanford-parser.jar&quot; os.environ[&quot;STANFORD_MODELS&quot;] = Utils.get_project_path()+&quot;\\lib_data\\stanford-parser-full-2018-02-27\\stanford-parser-3.9.1-models.jar&quot; self.sent_parser = stanford.StanfordParser( model_path=&quot;edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz&quot;, java_options=&apos;-mx8g&apos;, encoding=&apos;utf8&apos;) #&apos;-mlength100&apos; t1 = list(self.sent_parser.parse((senten,))]]></content>
      <categories>
        <category>python</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>stanford</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring装配]]></title>
    <url>%2F2019%2F01%2F21%2Fspring%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[Spring装配Bean Spring容器配置的三种主要的装配机制： 在XML中进行显示配置 在Java中进行显式配置 隐式的bean发现机制和自动装配 1-自动装配Spring从两个角度实现自动化装配： 组件扫描（component scanning） 自动发现应用上下文中所创建的bean 自动装配（autowiring） Spring自动满足bean之间的依赖 使用@Component注解来告诉Spring为这个类创建bean @Component public class CDPlayer implements MediaPlayer { private CompactDisc cd; @Autowired public CDPlayer(CompactDisc cd) { this.cd = cd; } public void play() { cd.play(); } } 通过设置组件扫描可以不用显示配置bean。但是组件扫描默认是不启用的。所以要通过配置来启用。 在java配置类中开启组件扫描@ComponentScan @Configuration @ComponentScan public class CDPlayerConfig { } 因为@ComponentScan会扫描与配置类相同的包。可以通过下面两个配置来设置扫描的包： @ComponentScan(basePackages={“soundsystem,”sdd”}) 扫描包的名字 @ComponentScan(basePackageClasses={CDplayer.class,DVDPlayer.class}) 扫描类所在的包 在XML中开启组件扫描 &lt;context:component-scan base-package=&quot;soundsystem&quot; /&gt; 为组件扫描的bean命名 通过@Component注解 @Component(&quot;sgtPerr&quot;) public class SgtPeppers implements CompactDisc { private String title = &quot;Sgt. Pepper&apos;s Lonely Hearts Club Band&quot;; private String artist = &quot;The Beatles&quot;; public void play() { System.out.println(&quot;Playing &quot; + title + &quot; by &quot; + artist); } } 通过@Named注解 @Named(&quot;sgtPerr&quot;) public class SgtPeppers implements CompactDisc { } 2- JAVA代码装配Bean通过配置类来实现装配Bean @Configuration //表明这是一个配置类 public class CDPlayerConfig { @Bean //声明这是一个bean public CompactDisc compactDisc() { return new SgtPeppers(); } @Bean //通过构造器注入 public CDPlayer cdPlayer(CompactDisc compactDisc) { return new CDPlayer(compactDisc); } @Bean //通过setter方法注入 public CDPlayer cdPlayer(CompactDisc compactDisc) { CDPlayer cdPlayer = new CDPlayer(compactDisc); cdPlayer.setCompactDisc(compactDisc); return cdPlayer } } 3- 通过XML装配bean 通过构造器注入 &lt;bean id=&quot;compactDisc&quot; class=&quot;soundsystem.collections.BlankDisc&quot;&gt; &lt;constructor-arg value=&quot;Sgt. Pepper&apos;s Lonely Hearts Club Band&quot; /&gt; &lt;constructor-arg value=&quot;The Beatles&quot; /&gt; &lt;constructor-arg&gt; &lt;list&gt; &lt;value&gt;Sgt. Pepper&apos;s Lonely Hearts Club Band&lt;/value&gt; &lt;value&gt;With a Little Help from My Friends&lt;/value&gt; &lt;value&gt;Lucy in the Sky with Diamonds&lt;/value&gt; &lt;value&gt;Getting Better&lt;/value&gt; &lt;value&gt;Fixing a Hole&lt;/value&gt; &lt;value&gt;She&apos;s Leaving Home&lt;/value&gt; &lt;value&gt;Being for the Benefit of Mr. Kite!&lt;/value&gt; &lt;value&gt;Within You Without You&lt;/value&gt; &lt;value&gt;When I&apos;m Sixty-Four&lt;/value&gt; &lt;value&gt;Lovely Rita&lt;/value&gt; &lt;value&gt;Good Morning Good Morning&lt;/value&gt; &lt;value&gt;Sgt. Pepper&apos;s Lonely Hearts Club Band (Reprise)&lt;/value&gt; &lt;value&gt;A Day in the Life&lt;/value&gt; &lt;/list&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; 通过setter注入 &lt;bean id=&quot;compactDisc&quot; class=&quot;soundsystem.properties.BlankDisc&quot;&gt; &lt;property name=&quot;title&quot; value=&quot;Sgt. Pepper&apos;s Lonely Hearts Club Band&quot; /&gt; &lt;property name=&quot;artist&quot; value=&quot;The Beatles&quot; /&gt; &lt;property name=&quot;tracks&quot;&gt; &lt;list&gt; &lt;value&gt;Sgt. Pepper&apos;s Lonely Hearts Club Band&lt;/value&gt; &lt;value&gt;With a Little Help from My Friends&lt;/value&gt; &lt;value&gt;Lucy in the Sky with Diamonds&lt;/value&gt; &lt;value&gt;Getting Better&lt;/value&gt; &lt;value&gt;Fixing a Hole&lt;/value&gt; &lt;value&gt;She&apos;s Leaving Home&lt;/value&gt; &lt;value&gt;Being for the Benefit of Mr. Kite!&lt;/value&gt; &lt;value&gt;Within You Without You&lt;/value&gt; &lt;value&gt;When I&apos;m Sixty-Four&lt;/value&gt; &lt;value&gt;Lovely Rita&lt;/value&gt; &lt;value&gt;Good Morning Good Morning&lt;/value&gt; &lt;value&gt;Sgt. Pepper&apos;s Lonely Hearts Club Band (Reprise)&lt;/value&gt; &lt;value&gt;A Day in the Life&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 4-混合配置 在javaConfig引用xml配置 @Configuration @Import(CDPlayerConfig.class) //导入其他的java配置 @ImportResource(&quot;classpath:cd-config.xml&quot;) // 导入xml配置 public class SoundSystemConfig { } 在xml配置中引用javaconfig &lt;bean class=&quot;soundsystem.CDConfig&quot; /&gt; 导入config配置类 &lt;import resource=&quot;cd-player-config.xml&quot;/&gt; 导入xml配置 ##高级装配 1- 环境与profile]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java多线程]]></title>
    <url>%2F2019%2F01%2F21%2Fjava%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Java多线程简单使用继承Thread类通过继承Thread，重写run方法 public class MyThread extends Thread{ @Override public void run() { super.run(); System.out.println(&quot;MyThread&quot;); } } 测试代码： @Test public void test1(){ MyThread myThread = new MyThread(); myThread.start(); System.out.println(&quot;运行结束&quot;); } 因为java是单根继承，不支持多继承，在程序设计上是有局限的。 实现Runnable接口（推荐）实现Runnale接口，重写run方法 public class MyRunnable implements Runnable { @Override public void run() { System.out.println(&quot;running&quot;); } } 测试代码： @Test public void test2(){ MyRunnable myRunnable = new MyRunnable(); Thread thread = new Thread(myRunnable); thread.start(); System.out.println(&quot;ending&quot;); } 注意，这里测试代码是实现Runnable接口后，通过Thread来接受Runnable接口对象，这样可以让Thread的start方法来调用Run方法，起到多线程的作用。 如果直接在主程序中调用run方法，就仅仅是程序的调用，无法起到多线程的作用。 java多线程的同步synchronize实现同步java中的每一个对象都可以作为锁，具体表现形式为下面三种形式： 对于普通同步方法，锁是当前实例对象 对于静态同步方法，锁是当前类的class对象 对于同步方法块，锁是Synchronize括号里配置的对象 上面三种形式默认synchronized相当于synchronized（this）锁this对象。 对于synchronized(非this对象x)这种情况，有下属三种结论： 当多个线程同时执行synchronized（x）{}同步代码块呈现同步效果。 当其它线程执行x对象中synchronized同步方法呈现同步效果 当其它线程执行x对象方法里面的synchronized（this）代码块也呈现同步效果。 volatile关键字关键字volatile主要作用是使变量在多个线程间可见 Java线程间通信等待/通知机制 等待/通知机制，是指一个线程A调用了对象O的wait()方法进入等待状态，而另一个线程B调用了对象O的notify()或者notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而执行后续操作。上述两个线程通过对象O来完成交互，而对象上的wait()和notify/notifyAll()的关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。 public class WaitNotify { static boolean flag = true; static Object lock = new Object(); public static void main(String[] args) throws Exception { Thread waitThread = new Thread(new Wait(), &quot;WaitThread&quot;); waitThread.start(); TimeUnit.SECONDS.sleep(1); Thread notifyThread = new Thread(new Notify(), &quot;NotifyThread&quot;); notifyThread.start(); } static class Wait implements Runnable { public void run() { // 加锁，拥有lock的Monitor synchronized (lock) { // 当条件不满足时，继续wait，同时释放了lock的锁 while (flag) { try { System.out.println(Thread.currentThread() + &quot; flag is true. wait @ &quot; + new SimpleDateFormat(&quot;HH:mm:ss&quot;).format(new Date())); lock.wait(); } catch (InterruptedException e) { } } // 条件满足时，完成工作 System.out.println(Thread.currentThread() + &quot; flag is false. running @ &quot; + new SimpleDateFormat(&quot;HH:mm:ss&quot;).format(new Date())); } } } static class Notify implements Runnable { public void run() { // 加锁，拥有lock的Monitor synchronized (lock) { // 获取lock的锁，然后进行通知，通知时不会释放lock的锁， // 直到当前线程释放了lock后，WaitThread才能从wait方法中返回 System.out.println(Thread.currentThread() + &quot; hold lock. notify @ &quot; + new SimpleDateFormat(&quot;HH:mm:ss&quot;).format(new Date())); lock.notifyAll(); flag = false; SleepUtils.second(5); } // 再次加锁 synchronized (lock) { System.out.println(Thread.currentThread() + &quot; hold lock again. sleep @ &quot; + new SimpleDateFormat(&quot;HH:mm:ss&quot;).format(new Date())); SleepUtils.second(5); } } } } 我们在使用这些方法需要注意下面几点： 1）使用wait()、notify()和notifyAll()时需要先对调用对象加锁。 即这些方法必须位于synchronize同步块内。 2）调用wait()方法后，线程状态由RUNNING变为WAITING，并将当前线程放置到对象的等待队列。 3）notify()或notifyAll()方法调用后，等待线程依旧不会从wait()返回，需要调用notify()或notifAll()的线程释放锁之后，等待线程才有机会从wait()返回。 即会将notify()或者notifAll()所在的同步块执行完才会调转到wait()所在的同步块。 4）notify()方法将等待队列中的一个等待线程从等待队列中移到同步队列中，而notifyAll()方法则是将等待队列中所有的线程全部移到同步队列，被移动的线程状态由WAITING变为BLOCKED。 5）从wait()方法返回的前提是获得了调用对象的锁。 管道输入/输出流管道流主要用于线程之间的数据传输，传输的媒介是内存。java中主要包括了4种具体的实现：PipedOutputStream、PipedInputStream、PipedReader和PipedWriter，前两种面向字节，而后两种面向字符。 public class Piped { public static void main(String[] args) throws Exception { PipedWriter out = new PipedWriter(); PipedReader in = new PipedReader(); // 将输出流和输入流进行连接，否则在使用时会抛出IOException out.connect(in); Thread printThread = new Thread(new Print(in), &quot;PrintThread&quot;); printThread.start(); int receive = 0; try { while ((receive = System.in.read()) != -1) { out.write(receive); } } finally { out.close(); } } static class Print implements Runnable { private PipedReader in; public Print(PipedReader in) { this.in = in; } public void run() { int receive = 0; try { while ((receive = in.read()) != -1) { System.out.print((char) receive); } } catch (IOException ex) { } } } } join方法 thread.join()当前线程等待thread线程终止后才继续执行。 thread.join(long millis)具备超时特性，在上者情况下，如果在指定时间内进程没终止，将从超时方法返回。 thread.join(long millis,int nanos) public class Join { public static void main(String[] args) throws Exception { Thread previous = Thread.currentThread(); for (int i = 0; i &lt; 10; i++) { // 每个线程拥有前一个线程的引用，需要等待前一个线程终止，才能从等待中返回 Thread thread = new Thread(new Domino(previous), String.valueOf(i)); thread.start(); previous = thread; } TimeUnit.SECONDS.sleep(5); System.out.println(Thread.currentThread().getName() + &quot; terminate.&quot;); } static class Domino implements Runnable { private Thread thread; public Domino(Thread thread) { this.thread = thread; } public void run() { try { thread.join(); } catch (InterruptedException e) { } System.out.println(Thread.currentThread().getName() + &quot; terminate.&quot;); } } } 结果如下： main terminate. 0 terminate. 1 terminate. 2 terminate. 3 terminate. 4 terminate. 5 terminate. 6 terminate. 7 terminate. 8 terminate. 9 terminate. ThreadLocal的使用ThreadLocal，即线程变量，是一个以ThreadLocal对象为键、任意对象为值的存储结构。这个结构被附带在线程上，也就是说一个线程可以根据一个ThreadLocal对象查询到绑定在这个线程上的一个值。 可以通过set(T)方法来设置一个值，在当前线程下再通过get()方法获取到原先设置的值。 public class Profiler { // 第一次get()方法调用时会进行初始化（如果set方法没有调用），每个线程会调用一次 private static final ThreadLocal&lt;Long&gt; TIME_THREADLOCAL = new ThreadLocal&lt;Long&gt;() { protected Long initialValue() { return System.currentTimeMillis();} }; public static final void begin() { TIME_THREADLOCAL.set(System.currentTimeMillis()); } public static final long end() { return System.currentTimeMillis() - TIME_THREADLOCAL.get(); } public static void main(String[] args) throws Exception { Profiler.begin(); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;Cost: &quot; + Profiler.end() + &quot; mills&quot;); } } java多线程基础线程优先级在Java线程中，通过一个整型成员变量priority来控制优先级，优先级的范围从1~10，在线程构建的时候可以通过setPriority(int)方法来修改优先级，默认优先级是5，优先级高的线程分配时间片的数量要多于优先级低的线程。设置线程优先级时，针对频繁阻塞（休眠或者I/O操作）的线程需要设置较高优先级，而偏重计算（需要较多CPU时间或者偏运算）的线程则设置较低的优先级，确保处理器不会被独占。在不同的JVM以及操作系统上，线程规划会存在差异，有些操作系统甚至会忽略对线程优先级的设定。 setPriority(int) 线程的状态java线程在运行的生命周期可能处于下表的6中不同的状态，在给定的一个时刻，线程只能处于其中的一个状态。 守护线程DaemonDaemon线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持性工作。这意味着，当一个Java虚拟机中不存在非Daemon线程的时候，Java虚拟机将会退出。可以通过调用Thread.setDaemon(true)将线程设置为Daemon线程。Daemon线程被用作完成支持性工作，但是在Java虚拟机退出时Daemon线程中的finally块并不一定会执行。 Daemon属性需要在启动线程之前设置，不能在启动线程之后设置 在构建Daemon线程时，不能依靠finally块中的内容来确保执行关闭或清理资源的逻辑。 线程中断使用interrupt()方法来停止线程，但interrupt()方法仅仅是在当前线程中刚打了一个断点。 java提供两种方法判断线程是否中断： this.interrupted()测试当前进程是否已经中断，执行后将具有状态标志清除为flase功能 this.isInterrupted()测试线程是否已经中断，不清除状态标志 推荐使用异常法来停止线程： public class MyThreadInterrupt extends Thread { @Override public void run() { super.run(); try{ for(int i=0;i&lt;500000;i++){ if(this.isInterrupted()){ System.out.println(&quot;线程终止，即将退出&quot;); throw new InterruptedException(); } System.out.println(&quot;i=&quot;+(i+1)); } System.out.println(&quot;for循环完了，继续执行程序。。。&quot;); }catch (InterruptedException e){ System.out.println(&quot;进入catch捕捉异常&quot;); e.printStackTrace(); } } } 测试代码： @Test public void test3(){ try{ MyThreadInterrupt myThreadInterrupt = new MyThreadInterrupt(); myThreadInterrupt.start(); Thread.sleep(2000); myThreadInterrupt.interrupt(); } catch (InterruptedException e){ System.out.println(&quot;mian catch&quot;); e.printStackTrace(); } } 如果不使用抛异常来终止线程，那么线程即使中断还是会for循环继续运行 或者使用标志位来控制是否需要停止任务并终止线程 public class Shutdown { public static void main(String[] args) throws Exception { Runner one = new Runner(); Thread countThread = new Thread(one, &quot;CountThread&quot;); countThread.start(); // 睡眠1秒，main线程对CountThread进行中断，使CountThread能够感知中断而结束 TimeUnit.SECONDS.sleep(1); countThread.interrupt(); Runner two = new Runner(); countThread = new Thread(two, &quot;CountThread&quot;); countThread.start(); // 睡眠1秒，main线程对Runner two进行取消，使CountThread能够感知on为false而结束 TimeUnit.SECONDS.sleep(1); two.cancel(); } private static class Runner implements Runnable { private long i; private volatile boolean on = true; @Override public void run() { while (on &amp;&amp; !Thread.currentThread().isInterrupted()) { i++; } System.out.println(&quot;Count i = &quot; + i); } public void cancel() { on = false; } } } 注意： suspend(),resume(),stop()已过期 全文来自 java并发编程的艺术 和 java多线程编程核心技术]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
</search>
